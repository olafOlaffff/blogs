{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Welcome to Ankamala Insights, where every data meets insight. Herein, we go deep into the stories lying beneath data, discover trends, derive meaningful conclusions, and share insights that actually matter. </p> <p>At Ankamala, it is all about creating stories from raw data. Be it the understanding of public data that's so vague and scattered, the latest press releases, finding a hidden pattern in any social issue, or actionable recommendations, we strive to ensure data reaches everyone in a way most accessible and impactful.</p>"},{"location":"#what-youll-find-here","title":"What You'll Find Here","text":"<ul> <li>Data-Driven Articles: Go through articles with actionable insights from unintelligibly complex data.</li> <li>Practical Applications: Follow how to implement data analysis on real problems, from public safety to social justice.</li> <li>Resources and tutorials: Description of tools and techniques that we apply to make sense out of large quantities of data, including advanced techniques.</li> </ul>"},{"location":"#our-mission","title":"Our Mission","text":"<p>It is our commitment to use data for driving positive social change. We focus on sharing keen, valuable analyses and insight that enable organizations, policymakers, and the people everywhere to make informed decisions in their pathway to creating change.</p>"},{"location":"NLP-Ankamala/","title":"Overview","text":""},{"location":"NLP-Ankamala/#what-is-nlp","title":"What is NLP?","text":"<p>Ever longed for your computer to be as up-to-date as that close friend of yours, who can easily get hold of your mood, your jokes, and even your secrets?</p> <p>Or, maybe you have wondered how Siri executes your command, or how she pops up a joke, or how she makes complex things sound so simple. You almost feel like having a super-friendly human chat with you, right? Humans are naturally curious, always eager to understand why things are happening as they are. Ever wondered - how?</p> <p>Now, let's dive deep into this fascinating concept and see how we at Ankamala bring this to life-improvement in our local community people using our very own NLP technology and helping serve and uplift thousands of lives toward awareness and justice. NLP is no longer a buzz-word or some rocket-science for 'techies'; it's for me, you, and everyone who aspires to utilize technology in order to make a good change in society.</p> <p>That is the magic of NLP: it allows machines to understand and respond to human languages in a manner that is all but incomprehensible. It changes this complex dance of words into a bridge between us and technology, making our interactions with computers natural and intuitive, just as talking to someone who understands you would be.</p>"},{"location":"NLP-Ankamala/#understanding-nlp-accuracy-applications-and-impact","title":"Understanding NLP - Accuracy, Applications, and Impact","text":"<p>At Ankamala, the quest for insightful data that brings change\u2014especially on social issues\u2014is what drives us. Recently, we analyzed the press releases of Nepal Police and compared them with independent data sources to understand sexual violence against minors. A sneak peek is given into our findings and also how NLP or Natural Language Processing plays a very important role in this analysis.</p>"},{"location":"NLP-Ankamala/#how-much-is-nlp-analysis-accurate","title":"How Much is NLP Analysis Accurate?","text":"<p>Some of the variables that could influence NLP accuracy are the quality of data and the sophistication of the algorithm involved. Our recent analysis of press releases from the Nepal Police gave us the following data:</p> <ul> <li>Data: According to official records, in the cases of sexual violence, victims came out to be 64.22% minor girls.</li> <li>Ankamala's NLP Analysis: From our analysis, using NLP, the victims are girls of 18 years or younger, which amount to about 64.53% of the victims. The breakdown is as follows:</li> <li>Age \u2264 16: 53.46%</li> <li>Age \u2264 14: 37.89%</li> <li>Age \u2264 12: 21.11%</li> </ul> <p>This comes pretty close to official figures; this is a very good validation of how well our NLP model has been able to extract and present the critical data.</p>"},{"location":"NLP-Ankamala/#nlp-in-data-acquisition-and-analysis","title":"NLP in Data Acquisition and Analysis","text":"<p>NLP allows us to create and investigate data in the following ways without having any authoritative data at hand:</p> <ol> <li> <p>Data Extraction: NLP helps extract data from a wide range of text sources, including press releases, articles, and reports. Such data is then studied to have trends and insights derived.</p> </li> <li> <p>Trend Analysis: With analytics on text data, NLP is able to show some trends that have changed over time. For instance, our analysis provides the age distribution of victims and compares them to already existing data.</p> </li> <li> <p>Granular Information: NLP can excavate more granular information, like the place of incidents\u2014whether it is at home, school, or the relative's house\u2014and the nature of threats\u2014Fear or Coercion\u2014that helps in planning awareness campaigns and interventions accordingly.</p> </li> <li> <p>Graphical Representation: NLP analysis can be plotted on graphs and charts. We plot graphs showing the age group distribution of victims. Similarly, we can study perpetrator demography based on NLP analysis.</p> </li> <li> <p>Campaign Design: NLP insights might lead to the construction of effective awareness campaigns. Knowing the pattern and details of the incident, one would frame better awareness campaigns.</p> </li> </ol>"},{"location":"NLP-Ankamala/#how-ankamala-can-help","title":"How Ankamala Can Help","text":"<p>NLP at Ankamala does not involve data analysis alone but applies this knowledge to provide actionable insights that will be useful for organizations working in the field of social and national issues. Detailed reports and analysis help in the following areas:</p> <ul> <li>Improved creation of awareness through designing appropriate campaigns based on correct data and insight</li> <li>Informed policy making through evidence-based recommendations provided to policymakers</li> <li>Better safety due to identification of high-risk areas and factors leading to incidents.</li> </ul> <p>Summary: This, in itself, makes NLP strong machinery for data processing and analytics. Proper interpretation yields valuable insights that go toward informed decisions to add value to society in general. At Ankamala, we are committed to leveraging NLP to create a meaningful impact in areas related to child protection and beyond.</p>"},{"location":"NLP-Basics-to-Advanced/Examples/","title":"Example Projects","text":""},{"location":"NLP-Basics-to-Advanced/Examples/#project-1-email-classification-whether-its-spam-or-not","title":"Project 1 - Email Classification whether it's spam or not.","text":"<ul> <li>(Using Average Word2Vec, NLTK &amp; Logistic Regression )</li> <li>Dataset taken from kaggle (emails.csv)</li> </ul> <pre><code>import pandas as pd\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom gensim.models import Word2Vec\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load dataset\ndata = pd.read_csv('emails.csv')\n\n# Sample the data to speed up processing\ndata = data.sample(n=1000, random_state=42)\n\n# Print initial shape and check for null values\nprint(f'Initial Data Shape: {data.shape}')\nprint(f'Null Values in Data: {data.isnull().sum()}')\n\n# Define the text preprocessing function\ndef preprocess_text(text):\n    text = text.lower()  # Convert to lowercase\n    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n    words = word_tokenize(text)  # Tokenize the text\n    words = [word for word in words if word not in stopwords.words('english')]  # Remove stopwords\n    return words\n\n# Apply preprocessing to the text\ndata['processed_text'] = data['text'].apply(preprocess_text)\n\n# Print the first few rows after preprocessing\nprint('\\nFirst Few Rows After Preprocessing:')\nprint(data[['text', 'processed_text']].head())\n\n# Train Word2Vec model\nsentences = data['processed_text'].tolist()\nw2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1)\n\n# Print information about the Word2Vec model\nprint(f'\\nWord2Vec Model Vocabulary Size: {len(w2v_model.wv)}')\n\n# Define function to calculate average word vectors\ndef avg_word2vec(words, model, num_features):\n    feature_vec = np.zeros((num_features,), dtype='float32')\n    num_words = 0\n    for word in words:\n        if word in model.wv:\n            num_words += 1\n            feature_vec = np.add(feature_vec, model.wv[word])\n    if num_words &gt; 0:\n        feature_vec = np.divide(feature_vec, num_words)\n    return feature_vec\n\n# Calculate average word vectors for each document\ndata['avg_vector'] = data['processed_text'].apply(lambda x: avg_word2vec(x, w2v_model, 100))\n\n# Print the first few rows with average vectors\nprint('\\nFirst Few Rows with Average Word Vectors:')\nprint(data[['processed_text', 'avg_vector']].head())\n\n# Convert 'avg_vector' column to a DataFrame and then to a NumPy array\nX = pd.DataFrame(data['avg_vector'].tolist()).to_numpy()\ny = data['spam']\n\n# Print shapes of feature matrix and target variable\nprint(f'\\nFeature Matrix Shape: {X.shape}')\nprint(f'Target Variable Shape: {y.shape}')\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Print shapes of training and testing sets\nprint(f'\\nTraining Feature Matrix Shape: {X_train.shape}')\nprint(f'Testing Feature Matrix Shape: {X_test.shape}')\nprint(f'Training Target Variable Shape: {y_train.shape}')\nprint(f'Testing Target Variable Shape: {y_test.shape}')\n\n# Train a Logistic Regression model\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'\\nAccuracy: {accuracy}')\n\n# Visualization\nplt.figure(figsize=(10, 6))\n\n# Plot distribution of average word vector dimensions\nplt.subplot(1, 2, 1)\nplt.title('Distribution of Average Word Vector Dimensions')\nplt.hist(X.flatten(), bins=30, edgecolor='k')\nplt.xlabel('Vector Dimension Value')\nplt.ylabel('Frequency')\n\n# Plot a sample of predicted vs. actual labels\nplt.subplot(1, 2, 2)\nplt.title('Predicted vs Actual Labels')\nplt.scatter(y_test, y_pred, alpha=0.5)\nplt.xlabel('Actual Label')\nplt.ylabel('Predicted Label')\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p>output:</p> <pre><code>$ python email-classification2.py\nInitial Data Shape: (1000, 2)\nNull Values in Data: text    0\nspam    0\ndtype: int64\n\nFirst Few Rows After Preprocessing:\n                                                   text                                     processed_text  \n4445  Subject: re : energy derivatives conference - ...  [subject, energy, derivatives, conference, may...  \n4118  Subject: financial maths course , part 2  vinc...  [subject, financial, maths, course, part, 2, v...  \n3893  Subject: re : bullet points  please respond to...  [subject, bullet, points, please, respond, hi,...  \n4210  Subject: re : enron default swaps  darrell ,  ...  [subject, enron, default, swaps, darrell, send...  \n5603  Subject: re : power question  steve ,  elena c...  [subject, power, question, steve, elena, chilk...  \n\nWord2Vec Model Vocabulary Size: 15317\n\nFirst Few Rows with Average Word Vectors:\n                                         processed_text                                         avg_vector  \n4445  [subject, energy, derivatives, conference, may...  [-0.23322825, 0.2750046, -0.4309876, -0.307644...  \n4118  [subject, financial, maths, course, part, 2, v...  [-0.24642478, 0.28744048, -0.40274242, -0.2679...  \n3893  [subject, bullet, points, please, respond, hi,...  [-0.025931308, 0.34718484, -0.48855108, -0.495...  \n4210  [subject, enron, default, swaps, darrell, send...  [-0.19965315, 0.27539662, -0.38174188, -0.2247...  \n5603  [subject, power, question, steve, elena, chilk...  [-0.20673649, 0.3041527, -0.49089968, -0.42844...  \n\nFeature Matrix Shape: (1000, 100)\nTarget Variable Shape: (1000,)\n\nTraining Feature Matrix Shape: (800, 100)\nTesting Feature Matrix Shape: (200, 100)\nTraining Target Variable Shape: (800,)\nTesting Target Variable Shape: (200,)\n\nAccuracy: 0.905\n</code></pre>"},{"location":"NLP-Basics-to-Advanced/Examples/#project-2-tweet-sentiment-analysis","title":"Project 2 - Tweet Sentiment Analysis","text":"<ul> <li>(Using Spacy and LogisticRegression)</li> <li>Dataset taken from kaggle (sentiment.csv)</li> </ul> <p><pre><code>import pandas as pd\nimport re\nimport spacy\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load spaCy's English model\nnlp = spacy.load('en_core_web_sm')\n\n# Load the dataset\nDATASET_COLUMNS = ['target', 'ids', 'date', 'flag', 'user', 'text']\nDATASET_ENCODING = \"ISO-8859-1\"\ndf = pd.read_csv('sentiment.csv', encoding=DATASET_ENCODING, names=DATASET_COLUMNS, header=None)\n\nprint(\"Dataset columns:\", df.columns)\n\n# Function to clean text using spaCy\ndef clean_text(text):\n    doc = nlp(text.lower())  # Convert to lowercase and process with spaCy\n\n    # Lemmatize, remove stopwords and punctuation\n    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct] \n    return \" \".join(tokens)\n\n# Preprocess the text data\ndf['text'] = df['text'].apply(clean_text)\n\n# Replace target values (4 becomes 1 for binary classification)\ndf['target'] = df['target'].replace(4, 1)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(df['text'], df['target'], test_size=0.2, random_state=42)\n\n# Initialize the TfidfVectorizer\nvectorizer = TfidfVectorizer(max_features=500000, ngram_range=(1, 2))\n\n# Fit and transform the training data\nX_train_vect = vectorizer.fit_transform(X_train)\n\n# Transform the testing data\nX_test_vect = vectorizer.transform(X_test)\n\n# Define the model evaluation function\ndef evaluate_model(model):\n    model.fit(X_train_vect, y_train)\n    y_pred = model.predict(X_test_vect)\n    print(\"Classification Report:\")\n    print(classification_report(y_test, y_pred))\n\n    # Confusion Matrix\n    cm = confusion_matrix(y_test, y_pred)\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.title('Confusion Matrix')\n    plt.show()\n\n# Train and evaluate the Logistic Regression model\nlr_model = LogisticRegression(C=2, max_iter=1000, n_jobs=-1)\nevaluate_model(lr_model)\n\n# Eg test tweet\ntest_tweet = [\"I just love this new idea! Proud of your decision. It's too good.\"]\n\n# Clean and transform the test tweet\ntest_tweet = [clean_text(test_tweet[0])]\nvectorized_tweet = vectorizer.transform(test_tweet)\n\n# Predict the sentiment\npredicted_sentiment = lr_model.predict(vectorized_tweet)\n\n# Output:\nprint(f\"Predicted Sentiment: {'Positive' if predicted_sentiment[0] == 1 else 'Negative'}\")\n</code></pre> Output: <pre><code>Dataset columns: Index(['target', 'ids', 'date', 'flag', 'user', 'text'], dtype='object')\n</code></pre> <pre><code>Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.90      0.85      0.87       1000\n           1       0.80      0.85      0.82       1000\n\n    accuracy                           0.83       2000\n   macro avg       0.85      0.83      0.85       2000\nweighted avg       0.85      0.83      0.84       2000\n</code></pre> <pre><code>Predicted Sentiment: Positive\n</code></pre></p>"},{"location":"NLP-Basics-to-Advanced/Lesson1/","title":"NLP Basics to Advanced","text":"<p>Natural Language Processing (NLP) is a critical field within artificial intelligence that bridges the gap between human communication and computer understanding. From enabling voice-activated assistants to powering sophisticated text analytics, NLP plays a pivotal role in making machines understand and respond to human language. This guide will take you through the foundational concepts of NLP, progressing to more advanced techniques, complete with detailed explanations and practical code examples.</p>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#1learning-flow","title":"1.Learning Flow","text":"<ol> <li>Learning Flow</li> <li> Why Machine Learning and Advanced AI Techniques Cannot Be Applied Alone</li> <li> <p>Why NLP Is Essential</p> </li> <li> <p>Roadmap to NLP</p> </li> <li>Cleaning the Input/Text<ul> <li>5.1 Cleaning</li> <li>5.2 Tokenization</li> <li>5.3 Stemming</li> <li>5.4 Lemmatization</li> </ul> </li> <li>Converting Input to Vectors<ul> <li>6.1 Vectorization Techniques I<ul> <li>i. Bag of Words</li> <li>ii. TF-IDF</li> <li>iii. N-grams</li> </ul> </li> <li>6.2 Vectorization Techniques II<ul> <li>i. Word2Vec</li> <li>ii. Average Word2Vec</li> </ul> </li> </ul> </li> <li>Advanced Vectorization Techniques<ul> <li>7.1 GloVe</li> <li>7.2 FastText</li> </ul> </li> <li>Part-of-Speech (POS) Tagging</li> <li>Text Classification</li> <li>Named Entity Recognition (NER)</li> <li>Sentiment Analysis</li> <li>Language Models and Transformers</li> <li>Practical Applications and Projects</li> </ol>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#2-why-machine-learning-and-advanced-ai-techniques-cannot-be-applied-alone","title":"2. Why Machine Learning and Advanced AI Techniques Cannot Be Applied Alone","text":"<p>While machine learning and other advanced AI techniques have greatly improved data analysis, when it comes down to minute details regarding human language, they always tend to break down. They would fail when it came to the subtlety of language with things like context and meaning-a place where NLP plays an important role.</p> <p>NLP is a branch of AI, committed to the development of techniques and tools that would allow computers to understand, interpret, and produce human language accurately. In contrast to general machine learning methods, NLP is concerned with the specifics of language, its structure and meaning.</p> <p>Machine Learning and other advanced AI techniques have taken the analysis of data further, yet it has not enabled the understanding of human language. Here's why:</p> <ul> <li>Contextual Understanding</li> </ul> <p>They do not capture the essence of the use of words. Human languages are highly context dependent, and this is often beyond the scope of training of ML models.</p> <ul> <li>Semantic Ambiguity</li> </ul> <p>Words take different meanings given different contexts. ML algorithms go haywire when it comes to dealing with this ambiguity, leading to misunderstandings.</p> <ul> <li>Syntax and Structure</li> </ul> <p>The grammar and syntax of human language may be complex and beyond the comprehension capability of ML models. This probably causes mistakes in the meaning obtained.</p> <ul> <li>Pragmatics</li> </ul> <p>This often makes them miss the implied meanings of most messages and social cues, hence the relevance of the responses.</p> <p>As much as MLs and advanced AI are strong, they usually have a lot of problems when it comes to the intricacies of human languages. Actually, NLP is supposed to help in solving such challenges in depth.</p>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#3-why-nlp-is-essential","title":"3. Why NLP Is Essential","text":"<p>NLP enhances various applications, from speech recognition to text analysis, by addressing critical aspects of language processing:</p> <ul> <li> <p>Contextual Understanding: NLP enables computers to grasp the meaning of words and sentences based on context, improving how well they interact with other meanings. This is crucial for effective speech recognition and text analysis.</p> </li> <li> <p>Semantic Analysis: NLP resolves ambiguities such as multiple meanings of words and synonyms, helping machines understand language more accurately.</p> </li> <li> <p>Natural Interaction: NLP facilitates smoother interactions between humans and machines, allowing chatbots and virtual assistants to generate responses that feel more 'human-like.'</p> </li> </ul> <p>These NLP capabilities refine language-based technologies, boosting their performance, accuracy, and contextual awareness in both speech and text. </p>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#nlp-models","title":"NLP Models","text":""},{"location":"NLP-Basics-to-Advanced/Lesson1/#introduction-to-nlp-models","title":"Introduction to NLP Models","text":""},{"location":"NLP-Basics-to-Advanced/Lesson1/#traditional-nlp-models","title":"Traditional NLP Models","text":"<ul> <li>Logistic Regression: A simple model used to classify text into categories, such as detecting whether a review is positive or negative.</li> <li>Naive Bayes: A probabilistic model that's effective for text classification tasks, assuming that features (words) are independent of each other (e.g., determining spam vs. non-spam emails).</li> <li>Support Vector Machines (SVM): A model that finds the best line (or hyperplane) to separate different categories of text, useful for tasks like sentiment analysis.</li> <li>Decision Trees: A model that makes decisions by splitting data into branches based on feature values, helping in text classification.</li> <li>K-Nearest Neighbors (KNN): A model that classifies text based on the most common category among its nearest neighbors in feature space.</li> </ul>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#feature-extraction-and-representation-techniques","title":"Feature Extraction and Representation Techniques","text":"<ul> <li>TF-IDF (Term Frequency-Inverse Document Frequency): A method to quantify the importance of words in a document relative to all documents. Useful for converting text into numerical data for machine learning models.</li> <li>Word Embeddings: Techniques like Word2Vec and GloVe convert words into dense, continuous vectors that capture their meanings and relationships. These embeddings are foundational for modern NLP models.</li> <li>Latent Dirichlet Allocation (LDA): A topic modeling technique that discovers hidden topics in a collection of documents by analyzing word distributions.</li> </ul>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#advanced-nlp-models","title":"Advanced NLP Models","text":"<ul> <li>Transformers: Cutting-edge models that use self-attention mechanisms to understand the context of each word in a sentence by considering all words simultaneously. This architecture is the foundation of many advanced NLP models.</li> <li>BERT (Bidirectional Encoder Representations from Transformers): Reads text in both directions (left-to-right and right-to-left) to provide a deep understanding of word meanings and context, improving performance on various NLP tasks.</li> <li>GPT (Generative Pre-trained Transformer): Generates coherent text by predicting the next word in a sequence, useful for applications like content generation and conversational agents.</li> <li>T5 (Text-To-Text Transfer Transformer): Treats every NLP task as a text-to-text problem, making it versatile for tasks like translation, summarization, and question answering.</li> <li>XLNet: Builds on BERT by using a permutation-based approach to better capture context and relationships between words, enhancing performance on several benchmarks.</li> <li>RoBERTa (Robustly optimized BERT approach): An improved version of BERT with better training techniques and data, leading to enhanced performance on a range of NLP tasks.</li> <li>ALBERT (A Lite BERT): A more efficient version of BERT, designed to be smaller and faster while maintaining similar performance.</li> <li>CLIP (Contrastive Language-Image Pretraining): Understands both text and images, enabling applications that require integrating visual and textual information, such as image captioning.</li> <li>DALL-E: Generates images from textual descriptions, combining text and visual creativity for generating unique images based on written prompts.</li> <li>LLaMA (Large Language Model Meta AI): A large-scale language model developed by Meta, designed for a variety of NLP tasks, including text generation and understanding, leveraging transformer architectures.</li> <li>Meta-Learning Models: Includes techniques like MAML (Model-Agnostic Meta-Learning) that allow models to quickly adapt to new tasks with minimal data.</li> </ul>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#4-roadmap-to-nlp","title":"4. Roadmap to NLP","text":"<p>To build a solid foundation in NLP, it's essential to follow a structured learning path. This roadmap outlines the key areas you should focus on, complete with detailed explanations and practical code examples.</p>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#5-cleaning-the-inputtext","title":"5. Cleaning the Input/Text","text":"<p>Before any meaningful analysis can be performed on text data, it's crucial to preprocess and clean the data. This involves several sub-tasks:</p>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#51-cleaning","title":"5.1. Cleaning","text":"<p>Objective: Remove unwanted characters, symbols, and noise from the text to prepare it for further processing.</p> <p>Common Cleaning Steps:</p> <ul> <li>Lowercasing: Convert all text to lowercase to ensure uniformity.</li> <li>Removing Punctuation: Eliminate punctuation marks that may not contribute to the analysis.</li> <li>Removing Numbers: Depending on the application, numbers might be irrelevant.</li> <li>Removing Stop Words: Filter out common words like \"the\", \"is\", \"in\" that do not carry significant meaning.</li> <li>Removing Whitespace: Trim unnecessary spaces.</li> </ul> <p>Code Example:</p> <p><pre><code>import re\nfrom nltk.corpus import stopwords\n\n# Sample text\ntext = \"Hello! Welcome to NLP Basics. Let's clean this text: remove numbers 123, punctuation!!!\"\n\n# Lowercasing\ntext = text.lower()\n\n# Removing punctuation and numbers\ntext = re.sub(r'[^a-z\\s]', '', text)\n\n# Removing stop words\nstop_words = set(stopwords.words('english'))\ntokens = text.split()\nfiltered_tokens = [word for word in tokens if word not in stop_words]\n\n# Rejoining tokens\ncleaned_text = ' '.join(filtered_tokens)\nprint(\"Cleaned Text:\", cleaned_text)\n</code></pre> Expected Output: <pre><code>$ python Lesson1.py:\nCleaned Text: hello welcome nlp basics lets clean text remove numbers punctuation marks\n</code></pre> </p>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#52-tokenization","title":"5.2. Tokenization","text":"<p>Objective: Split the cleaned text into individual units called tokens, which can be words or sentences.</p> <p>Types of Tokenization:</p> <ul> <li>Word Tokenization: Divides text into individual words.</li> <li>Sentence Tokenization: Divides text into individual sentences.</li> </ul> <p><pre><code>import nltk\nfrom nltk.tokenize import word_tokenize, sent_tokenize\n\n# Sample text\ntext = \"Hello! Welcome to NLP Basics. Let's tokenize this text.\"\n\n# Sentence Tokenization\nsentences = sent_tokenize(text)\nprint(\"Sentence Tokenization:\", sentences)\n\n&lt;br/&gt;\n# Word Tokenization\nwords = word_tokenize(text)\nprint(\"Word Tokenization:\", words)\n</code></pre> Expected Output: <pre><code>Sentence Tokenization: ['Hello!', 'Welcome to NLP Basics.', \"Let's tokenize this text.\"]\nWord Tokenization: ['Hello', '!', 'Welcome', 'to', 'NLP', 'Basics', '.', 'Let', \"'s\", 'tokenize', 'this', 'text', '.']\n</code></pre> </p>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#53-stemming","title":"5.3 Stemming","text":"<p>Stemming reduces words to their base or root form. This technique can be useful in reducing word variations for analysis.</p> <p><pre><code>from nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_words = [stemmer.stem(word) for word in tokens]\n\nprint(\"Stemmed Words:\", stemmed_words)\n</code></pre> Expected Output: <pre><code>Stemmed Words: ['hello', 'thi', 'is', 'a', 'sampl', 'text', 'with', 'number', 'and', 'symbol']\n</code></pre> </p>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#54-lemmatization","title":"5.4 Lemmatization","text":"<p>Lemmatization reduces words to their dictionary form, also known as the lemma, considering the context. <pre><code>from nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\nlemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n\nprint(\"Lemmatized Words:\", lemmatized_words)\n</code></pre> Expected Output: <pre><code>Lemmatized Words: ['hello', 'this', 'is', 'a', 'sample', 'text', 'with', 'number', 'and', 'symbol']\n</code></pre> </p>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#6-converting-input-to-vectors","title":"6. Converting Input to Vectors","text":""},{"location":"NLP-Basics-to-Advanced/Lesson1/#-importance-of-vectorization-in-text-classification","title":"- Importance of Vectorization in Text Classification","text":"<ul> <li>Numerical Input: Converts text to numerical data for machine learning algorithms.</li> <li>Feature Encoding: Represents text with vectors, capturing term frequency and context.</li> <li>Dimensionality Reduction: Manages large vocabularies efficiently.</li> </ul> <p>Raw text cannot be processed by algorithms; vectorization enables computational operations on text data.</p> <p> Vectorization is essential for effective text processing, especially in text classification tasks. Here\u2019s a comparison between manual text comparison methods and vectorization.</p>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#-examplesentiment-classification","title":"- Example:Sentiment Classification","text":"<p>Goal: Classify a new movie review as \"positive\" or \"negative\" based on similarity to existing reviews. </p>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#-without-vectorization","title":"-Without Vectorization","text":"<p>Manual string comparison:</p> <p><pre><code># Sample movie reviews and their labels\nreviews = [\n    \"I love this movie\",\n    \"The movie was terrible\",\n    \"Fantastic film!\",\n    \"I hated the film\"\n]\nlabels = ['positive', 'negative', 'positive', 'negative']\n\n# New review to classify\nnew_review = \"The film was great\"\n\n# Manual similarity check\ndef classify_review_manual(new_review, reviews, labels):\n    for i, review in enumerate(reviews):\n        if review.lower() in new_review.lower():\n            return labels[i]\n    return \"unknown\"\n\n# Classify using manual check\npredicted_label_manual = classify_review_manual(new_review, reviews, labels)\nprint(\"Manual Classification:\", predicted_label_manual)\n</code></pre> Expected Output: <pre><code>Manual Classification: unknown\n</code></pre> </p>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#-with-vectorization","title":"- With Vectorization","text":"<p>Using vectorization to handle text:</p> <p><pre><code>import nltk\nfrom nltk.util import ngrams\nfrom collections import Counter\n\n# Function to create bigrams (pairs of consecutive words)\ndef generate_bigrams(text):\n    words = text.lower().split()\n    return [' '.join(gram) for gram in ngrams(words, 2)]\n\n# Function to create bigram features from corpus\ndef create_bigram_features(corpus):\n    all_bigrams = [bigram for text in corpus for bigram in generate_bigrams(text)]\n    bigram_counts = Counter(all_bigrams)\n    return list(bigram_counts.keys())\n\n# Function to convert text into a bigram vector\ndef text_to_vector(text, feature_names):\n    text_bigrams = generate_bigrams(text)\n    text_bigram_counts = Counter(text_bigrams)\n    return [text_bigram_counts.get(bigram, 0) for bigram in feature_names]\n\n# Function to classify new text based on similarity using vectors\ndef classify_text(new_text, training_vectors, training_labels):\n    new_vector = text_to_vector(new_text, feature_names)\n    similarities = [sum(a * b for a, b in zip(new_vector, vec)) for vec in training_vectors]\n    return training_labels[similarities.index(max(similarities))]\n\n# Sample movie reviews and their labels\nreviews = [\n    \"I love this movie\",\n    \"The movie was terrible\",\n    \"Fantastic film!\",\n    \"I hated the film\"\n]\nlabels = ['positive', 'negative', 'positive', 'negative']\n\n# Create feature names from corpus\nfeature_names = create_bigram_features(reviews)\n\n# Convert reviews to vectors\nX = [text_to_vector(text, feature_names) for text in reviews]\n\n# New review to classify\nnew_review = \"The film was great\"\n\n# Classify using vectorization\npredicted_label_vector = classify_text(new_review, X, labels)\nprint(\"Vectorized Classification:\", predicted_label_vector)\n</code></pre> Expected Output: <pre><code>Vectorized Classification: positive\n</code></pre></p> <p>Explanation: Vectorization converts text to numerical vectors, allowing effective similarity measurement. The new review \"The film was great\" is compared against existing review vectors, finding the most similar review (\"Fantastic film!\") and assigning the same label, which is \"positive.\" </p>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#summary","title":"Summary","text":"<p>Without Vectorization: Limited to exact matches; impractical for large or diverse datasets. With Vectorization: Understands context ;scalable and effective for complex text analyses.</p> <p> Once the text is cleaned and tokenized, it must be converted into numerical vectors to be processed by machine learning algorithms. There are two broad approaches for text vectorization:</p>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#61-vectorization-techniques-i-traditional-approaches","title":"6.1 Vectorization Techniques I (Traditional Approaches)","text":""},{"location":"NLP-Basics-to-Advanced/Lesson1/#i-bag-of-words-bow","title":"i. Bag of Words (BoW)","text":"<p>What it does: The Bag of Words model converts text into a matrix of word frequencies or binary occurrence.</p> <p>Code Example:</p> <p><pre><code>from collections import Counter\nfrom nltk.tokenize import word_tokenize\nimport nltk\nnltk.download('punkt')\n\ndef text_to_vector(text):\n    tokens = word_tokenize(text.lower())\n    return Counter(tokens)\n\ntext = \"I love learning NLP from Ankamala tutorials\"\nvector = text_to_vector(text)\nprint(\"Bag of Words Vector:\", vector)\n</code></pre> Expected Output: <pre><code>Bag of Words Vector: Counter({'i': 1, 'love': 1, 'learning': 1, 'nlp': 1, 'from': 1, 'ankamala': 1, 'tutorials': 1})\n</code></pre> </p>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#ii-term-frequency-inverse-document-frequency-tf-idf","title":"ii. Term Frequency-Inverse Document Frequency (TF-IDF)","text":"<p>TF-IDF highlights important words in a document by considering how often they appear across multiple documents.</p> <p>What it does: TF-IDF scores words higher if they are frequent in one document but rare across others.</p> <p>Code Example:</p> <p><pre><code>from sklearn.feature_extraction.text import TfidfVectorizer\n\ncorpus = [\"This is a sample sentence\", \"This is another sentence\"]\ntfidf = TfidfVectorizer()\nresult = tfidf.fit_transform(corpus)\n\nprint(\"Words:\", tfidf.get_feature_names_out())\nprint(\"TF-IDF Scores:\\n\", result.toarray())\n</code></pre> Expected Output: <pre><code>words: ['another', 'is', 'sample', 'sentence', 'this']\nTF-IDF Scores: [[0.  0.38  0.65  0.38  0.65]  [0.65  0.38  0.  0.38  0.65]]\n</code></pre> </p>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#iii-n-grams","title":"iii. N-Grams","text":"<p>An N-gram is a sequence of 'n' words in a sentence. N-grams help capture the context of words better than individual words (unigrams).</p> <p>What it does: N-grams capture word context by analyzing sequences of 'n' words, which improves tasks like text prediction, machine translation, and sentiment analysis.</p> <p>For example, consider the sentence: <code>\"I love learning NLP from Ankamala tutorials.\"</code></p> <ul> <li>Unigram (1-gram): Single words</li> <li> <p><code>[\"I\", \"love\", \"learning\", \"NLP\", \"from\", \"Ankamala\", \"tutorials\"]</code></p> </li> <li> <p>Bigram (2-gram): Pairs of consecutive words</p> </li> <li> <p><code>[(\"I\", \"love\"), (\"love\", \"learning\"), (\"learning\", \"NLP\"), (\"NLP\", \"from\"), (\"from\", \"Ankamala\"), (\"Ankamala\", \"tutorials\")]</code></p> </li> <li> <p>Trigram (3-gram): Triplets of consecutive words</p> </li> <li><code>[(\"I\", \"love\", \"learning\"), (\"love\", \"learning\", \"NLP\"), (\"learning\", \"NLP\", \"from\"), (\"NLP\", \"from\", \"Ankamala\"), (\"from\", \"Ankamala\", \"tutorials\")]</code></li> </ul> <p>Code Example:</p> <p><pre><code>from nltk.util import ngrams\nfrom nltk.tokenize import word_tokenize\nfrom collections import Counter\nimport nltk\nnltk.download('punkt')\n\ndef generate_ngrams(text, n):\n    tokens = word_tokenize(text.lower())\n    return list(ngrams(tokens, n))\n\ndef ngram_to_vector(text, n):\n    n_grams = generate_ngrams(text, n)\n    n_gram_freq = Counter(n_grams)\n    return n_gram_freq\n\n# Example text\ntext = \"I love learning NLP from Ankamala tutorials\"\n\n# Generate bigrams and trigrams\nbigrams = ngram_to_vector(text, 2)\ntrigrams = ngram_to_vector(text, 3)\n\nprint(\"Bigrams Vector:\", bigrams)\nprint(\"Trigrams Vector:\", trigrams)\n</code></pre> Expected Output: <pre><code>Bigrams Vector: Counter({('i', 'love'): 1, ('love', 'learning'): 1, ('learning', 'nlp'): 1, ('nlp', 'from'): 1, ('from', 'ankamala'): 1, ('ankamala', 'tutorials'): 1})\nTrigrams Vector: Counter({('i', 'love', 'learning'): 1, ('love', 'learning', 'nlp'): 1, ('learning', 'nlp', 'from'): 1, ('nlp', 'from', 'ankamala'): 1, ('from', 'ankamala', 'tutorials'): 1})\n</code></pre> </p>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#62-converting-input-to-vector-ii-advanced-approaches","title":"6.2 Converting Input to Vector II (Advanced Approaches)","text":""},{"location":"NLP-Basics-to-Advanced/Lesson1/#i-word2vec-iiaverage-word2vec","title":"i. Word2Vec   \u2003\u2003     ii.Average Word2Vec","text":"<p>i.Word2Vec   - Represents words in a vector space where similar words have closer vector representations.</p>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#why-use-word2vec","title":"Why Use Word2Vec?","text":"<p>Word2Vec helps us understand the meaning of words by representing them as vectors in a space where similar words are close together. Unlike simpler methods like TF-IDF, Word2Vec captures word similarities and relationships better, making it useful for various NLP tasks.</p>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#how-it-works","title":"How it Works","text":"<p>Speech recognition systems convert spoken language into text. Word embeddings like Word2Vec help these systems understand the context and meaning of the words in the text, making the recognition process more accurate.</p> <ul> <li>Introduction to Word Embeddings</li> </ul> <p>Word embeddings are vector representations of words that capture their meanings and relationships. They\u2019re crucial for understanding language in a more nuanced way.</p> <ul> <li>Word2Vec with Gensim</li> </ul> <p>Gensim is a powerful library for working with Word2Vec. Here\u2019s how you can use it:</p> <p>Install Gensim: <pre><code>pip install gensim\n</code></pre> Code Example: <pre><code>from gensim.models import Word2Vec\nfrom gensim.utils import simple_preprocess\n\n# Sample sentences\nsentences = [\n    \"This is a sample sentence.\",\n    \"This is another example of a sentence.\",\n    \"Natural language processing is fun.\",   \n]\n\n# Tokenize and preprocess sentences\npreprocessed_sentences = [simple_preprocess(sentence) for sentence in sentences]\n\n# Train Word2Vec model\nmodel = Word2Vec(sentences=preprocessed_sentences, vector_size=50, window=5, min_count=1, sg=0)\n\n# Get word vectors\nword_vectors = model.wv\n\n# Display vector for a word\nprint(\"Vector for 'sentence':\", word_vectors['sentence'])\n\n# Find similar words\nsimilar_words = word_vectors.most_similar('sentence', topn=3)\nprint(\"Words similar to 'sentence':\", similar_words)\n</code></pre> Expected Output: <pre><code>Vector for 'sentence': [ 0.0132551  -0.0017596  -0.0068934  -0.01502807  0.00064916  0.01293842\n -0.00333229 ....]\n\nWords similar to 'sentence': [('sample', 0.998260498046875), ('another', 0.9979914426803589), ('example', 0.9976629014015198)]\n</code></pre></p>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#ii-average-word2vec","title":"ii. Average Word2Vec","text":"<p>Average Word2Vec is a technique that creates a document-level embedding by averaging the individual word vectors generated by the Word2Vec model. It\u2019s a simple and effective way to represent entire sentences, paragraphs, or even documents as a single vector. The main idea is to take each word's vector and compute their mean, which provides a low-dimensional representation of the entire text.</p>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#how-it-works_1","title":"How It Works:","text":"<ol> <li> <p>Train Word2Vec: First, you train a Word2Vec model to generate word embeddings for individual words in your corpus. Word2Vec creates dense vector representations that capture semantic relationships between words.</p> </li> <li> <p>Compute Averages: For a given text (sentence or document), the vectors for all the words are averaged to produce a single, fixed-length vector that represents the entire text. This method can be useful for text classification, clustering, and similarity tasks.</p> </li> </ol>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#why-use-average-word2vec-instead-of-just-word2vec","title":"Why Use Average Word2Vec Instead of Just Word2Vec?","text":"<p>Word2Vec is great for representing individual words, but it has limitations when dealing with longer text like sentences or paragraphs. Here's how Average Word2Vec helps:</p> <p>1. Captures Sentence/Paragraph Meaning  - Word2Vec gives a vector for each word. - Average Word2Vec takes the average of all word vectors to represent the overall meaning of the sentence or paragraph.</p> <p>2. Handles Variable-Length Text  - Word2Vec produces multiple word vectors, making it hard to process longer texts. - Average Word2Vec gives a single, fixed-size vector for any length of text, making it easier to use in machine learning models.</p> <p>3. Improves Context Understanding  - Word2Vec only focuses on individual words, missing out on the full sentence meaning. - Average Word2Vec combines word vectors, capturing the overall meaning and context of the entire sentence.</p>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#code-example","title":"Code Example","text":"<p><pre><code>import numpy as np\nfrom gensim.downloader import load\n\n# Load a smaller pre-trained model (GloVe model)\nmodel = load('glove-wiki-gigaword-50')\n\n# Example sentences\nsentences = [\n    \"Have a Great day ahead!\",\n    \"This is not so good.\",\n    \"What a wonderful experience, highly recommend it!\",\n    \"Not worth watching, it's just waste of time.\"\n]\n\ndef get_average_vector(sentence):\n    # Tokenize the sentence into words\n    words = sentence.lower().split()\n    # Get vectors for words that are in the model\n    vectors = [model[word] for word in words if word in model]\n    # Compute the average vector\n    if vectors:\n        return np.mean(vectors, axis=0)\n    else:\n        return np.zeros(model.vector_size)\n\n# Compute average vectors for each sentence\nfor sentence in sentences:\n    avg_vector = get_average_vector(sentence)\n    print(f\"Sentence: '{sentence}'\")\n    print(f\"Average Vector: {avg_vector[:5]}...\")  # Print the first 5 dimensions\n    print()\n</code></pre> Output: <pre><code>$ python avg_w2vec.py \nSentence: 'Have a Great day ahead!'\nAverage Vector: [ 0.31396326  0.497535   -0.35236502 -0.1813525   0.52546036]...\n\nSentence: 'This is not so good.'\nAverage Vector: [ 0.5756425   0.1185125  -0.1963629  -0.07140501  0.58780503]...\n\nSentence: 'What a wonderful experience, highly recommend it!'\nAverage Vector: [ 0.414402    0.1216162  -0.546574   -0.23427948  0.65545803]...\n\nSentence: 'Not worth watching, it's just waste of time.'\nAverage Vector: [ 0.44660002  0.10159819  0.14150828 -0.098243    0.478798  ]...\n</code></pre></p>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#7-advanced-vectorization-techniques","title":"7. Advanced Vectorization Techniques","text":""},{"location":"NLP-Basics-to-Advanced/Lesson1/#71-glove","title":"7.1 GloVe","text":"<p>GloVe (Global Vectors for Word Representation) is a word embedding technique that captures semantic relationships between words by analyzing global word co-occurrence statistics in a corpus. Unlike Word2Vec, which focuses on local context (words surrounding the target word), GloVe looks at the broader picture, considering the entire corpus to build dense word vectors.</p> <p>How It Works:</p> <ul> <li>GloVe creates a co-occurrence matrix where each element represents how frequently a pair of words appears together in a context window.</li> <li>The matrix is then factorized to derive word embeddings, ensuring the vectors capture both global and local context.</li> </ul> <p>Advantages: - Combines the benefits of both matrix factorization and local context-based models. - Generates word vectors that reflect meaningful relationships like \"king - man + woman = queen.\"</p>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#8-part-of-speech","title":"8. Part of Speech","text":""},{"location":"NLP-Basics-to-Advanced/Lesson1/#pos-tagging","title":"POS Tagging","text":"<ul> <li>What POS Tagging Does: It labels each word in a sentence with its role, like whether it's a noun, verb, or adjective.</li> <li>Why It\u2019s Useful: It helps us understand the grammatical structure of sentences.  How It Helps in NLP:   -- Understanding Sentences: Makes sense of how words work together.   -- Supports Other Tasks: <ul> <li>Finding Names and Places: Helps spot important names and dates.</li> <li>Breaking Down Sentences: Helps analyze and understand sentence structure.</li> <li>Translating Text: Makes translating text into other languages more accurate.</li> </ul> </li> </ul> <p><pre><code>import spacy\n\n# Load the spaCy model\nnlp = spacy.load('en_core_web_sm')\n\n# Sample sentence\nsentence = \"Ankamala is a great platform to learn NLP.\"\n\n# Process the sentence with spaCy\ndoc = nlp(sentence)\n\n# Extract token and POS tags\npos_tags = [(token.text, token.pos_) for token in doc]\n\nprint(pos_tags)\n</code></pre> Output:</p> <pre><code>$ python pos_tag.py\n[('Ankamala', 'PROPN'), ('is', 'AUX'), ('a', 'DET'),\n ('great', 'ADJ'), ('platform', 'NOUN'), ('to', 'PART'), \n ('learn', 'VERB'), ('NLP', 'PROPN'), ('.', 'PUNCT')]\n</code></pre>"},{"location":"NLP-Basics-to-Advanced/NLP-Start/","title":"NLP-Basics-to-Advanced","text":""},{"location":"NLP-Basics-to-Advanced/NLP-Start/#nlp-basics-to-advanced","title":"NLP Basics to Advanced","text":"<p>Natural Language Processing, or NLP, is a field of artificial intelligence with a particular concentration on interaction between computers and humans through natural language.  This is a huge-spectrum field that undertakes everything from simple text processing to complex understanding and generation of human languages. Whether one is a complete beginner or an experienced practitioner in this field, the following guide will take one through the key concepts and techniques in NLP, from the most basic ones up to the really advanced. Among other things, our guide shall cover concepts along with code-snippets for quick implementations of acquired knowledge:</p> <p>In this guide, we will cover:</p> <ul> <li>Introduction to NLP: A deeper look at what NLP is and why it is important in today's world.</li> <li>Basic Text Processing:  Understand the cleaning, tokenization, and preprocessing of text data. </li> <li>Part-of-Speech Tagging and Named Entity Recognition: Explore the techniques to identify parts of speech and named entities within a text.</li> <li>Sentiment Analysis: Sentiment Analysis Understand how one may make an estimate of the sentiment within a given text</li> <li>Language Models and Word Embeddings: This module will explore word embeddings, TF-IDF, and some of the pre-trained models like Word2Vec, GloVe, and BERT.</li> <li>Advanced NLP Techniques: Discover deep learning techniques in NLP, such as recurrent neural networks (RNNs), transformers, and sequence-to-sequence models.</li> </ul> <p>By the end, you'll have a very solid grasp of NLP and how to approach a large variety of different real-world NLP tasks.</p>"}]}
{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Welcome to Ankamala Insights, where every data meets insight. Herein, we go deep into the stories lying beneath data, discover trends, derive meaningful conclusions, and share insights that actually matter. </p> <p>At Ankamala, it is all about creating stories from raw data. Be it the understanding of public data that's so vague and scattered, the latest press releases, finding a hidden pattern in any social issue, or actionable recommendations, we strive to ensure data reaches everyone in a way most accessible and impactful.</p>"},{"location":"#what-youll-find-here","title":"What You'll Find Here","text":"<ul> <li>Data-Driven Articles: Go through articles with actionable insights from unintelligibly complex data.</li> <li>Practical Applications: Follow how to implement data analysis on real problems, from public safety to social justice.</li> <li>Resources and tutorials: Description of tools and techniques that we apply to make sense out of large quantities of data, including advanced techniques.</li> </ul>"},{"location":"#our-mission","title":"Our Mission","text":"<p>It is our commitment to use data for driving positive social change. We focus on sharing keen, valuable analyses and insight that enable organizations, policymakers, and the people everywhere to make informed decisions in their pathway to creating change.</p>"},{"location":"NLP-Ankamala/","title":"Overview","text":""},{"location":"NLP-Ankamala/#what-is-nlp","title":"What is NLP?","text":"<p>Ever longed for your computer to be as up-to-date as that close friend of yours, who can easily get hold of your mood, your jokes, and even your secrets?</p> <p>Or, maybe you have wondered how Siri executes your command, or how she pops up a joke, or how she makes complex things sound so simple. You almost feel like having a super-friendly human chat with you, right? Humans are naturally curious, always eager to understand why things are happening as they are. Ever wondered - how?</p> <p>Now, let's dive deep into this fascinating concept and see how we at Ankamala bring this to life-improvement in our local community people using our very own NLP technology and helping serve and uplift thousands of lives toward awareness and justice. NLP is no longer a buzz-word or some rocket-science for 'techies'; it's for me, you, and everyone who aspires to utilize technology in order to make a good change in society.</p> <p>That is the magic of NLP: it allows machines to understand and respond to human languages in a manner that is all but incomprehensible. It changes this complex dance of words into a bridge between us and technology, making our interactions with computers natural and intuitive, just as talking to someone who understands you would be.</p>"},{"location":"NLP-Ankamala/#understanding-nlp-accuracy-applications-and-impact","title":"Understanding NLP - Accuracy, Applications, and Impact","text":"<p>At Ankamala, the quest for insightful data that brings change\u2014especially on social issues\u2014is what drives us. Recently, we analyzed the press releases of Nepal Police and compared them with independent data sources to understand sexual violence against minors. A sneak peek is given into our findings and also how NLP or Natural Language Processing plays a very important role in this analysis.</p>"},{"location":"NLP-Ankamala/#how-much-is-nlp-analysis-accurate","title":"How Much is NLP Analysis Accurate?","text":"<p>Some of the variables that could influence NLP accuracy are the quality of data and the sophistication of the algorithm involved. Our recent analysis of press releases from the Nepal Police gave us the following data:</p> <ul> <li>Data: According to official records, in the cases of sexual violence, victims came out to be 64.22% minor girls.</li> <li>Ankamala's NLP Analysis: From our analysis, using NLP, the victims are girls of 18 years or younger, which amount to about 64.53% of the victims. The breakdown is as follows:</li> <li>Age \u2264 16: 53.46%</li> <li>Age \u2264 14: 37.89%</li> <li>Age \u2264 12: 21.11%</li> </ul> <p>This comes pretty close to official figures; this is a very good validation of how well our NLP model has been able to extract and present the critical data.</p>"},{"location":"NLP-Ankamala/#nlp-in-data-acquisition-and-analysis","title":"NLP in Data Acquisition and Analysis","text":"<p>NLP allows us to create and investigate data in the following ways without having any authoritative data at hand:</p> <ol> <li> <p>Data Extraction: NLP helps extract data from a wide range of text sources, including press releases, articles, and reports. Such data is then studied to have trends and insights derived.</p> </li> <li> <p>Trend Analysis: With analytics on text data, NLP is able to show some trends that have changed over time. For instance, our analysis provides the age distribution of victims and compares them to already existing data.</p> </li> <li> <p>Granular Information: NLP can excavate more granular information, like the place of incidents\u2014whether it is at home, school, or the relative's house\u2014and the nature of threats\u2014Fear or Coercion\u2014that helps in planning awareness campaigns and interventions accordingly.</p> </li> <li> <p>Graphical Representation: NLP analysis can be plotted on graphs and charts. We plot graphs showing the age group distribution of victims. Similarly, we can study perpetrator demography based on NLP analysis.</p> </li> <li> <p>Campaign Design: NLP insights might lead to the construction of effective awareness campaigns. Knowing the pattern and details of the incident, one would frame better awareness campaigns.</p> </li> </ol>"},{"location":"NLP-Ankamala/#how-ankamala-can-help","title":"How Ankamala Can Help","text":"<p>NLP at Ankamala does not involve data analysis alone but applies this knowledge to provide actionable insights that will be useful for organizations working in the field of social and national issues. Detailed reports and analysis help in the following areas:</p> <ul> <li>Improved creation of awareness through designing appropriate campaigns based on correct data and insight</li> <li>Informed policy making through evidence-based recommendations provided to policymakers</li> <li>Better safety due to identification of high-risk areas and factors leading to incidents.</li> </ul> <p>Summary: This, in itself, makes NLP strong machinery for data processing and analytics. Proper interpretation yields valuable insights that go toward informed decisions to add value to society in general. At Ankamala, we are committed to leveraging NLP to create a meaningful impact in areas related to child protection and beyond.</p>"},{"location":"NLP-Basics-to-Advanced/Lesson1/","title":"NLP Basics to Advanced","text":"<p>Natural Language Processing (NLP) is a critical field within artificial intelligence that bridges the gap between human communication and computer understanding. From enabling voice-activated assistants to powering sophisticated text analytics, NLP plays a pivotal role in making machines understand and respond to human language. This guide will take you through the foundational concepts of NLP, progressing to more advanced techniques, complete with detailed explanations and practical code examples.</p>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#1learning-flow","title":"1.Learning Flow","text":"<ol> <li>Learning Flow</li> <li> Why Machine Learning and Advanced AI Techniques Cannot Be Applied Alone</li> <li> <p>Why NLP Is Essential</p> </li> <li> <p>Roadmap to NLP</p> </li> <li>Cleaning the Input/Text<ul> <li>5.1 Cleaning</li> <li>5.2 Tokenization</li> <li>5.3 Stemming</li> <li>5.4 Lemmatization</li> </ul> </li> <li>Converting Input to Vectors<ul> <li>6.1 Vectorization Techniques I<ul> <li>i. Bag of Words</li> <li>ii. TF-IDF</li> <li>iii. N-grams</li> </ul> </li> <li>6.2 Vectorization Techniques II<ul> <li>i. Word2Vec</li> <li>ii. Average Word2Vec</li> </ul> </li> </ul> </li> <li>Advanced Vectorization Techniques<ul> <li>7.1 GloVe</li> <li>7.2 FastText</li> </ul> </li> <li>Part-of-Speech (POS) Tagging</li> <li>Text Classification</li> <li>Named Entity Recognition (NER)</li> <li>Sentiment Analysis</li> <li>Language Models and Transformers</li> <li>Practical Applications and Projects</li> </ol>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#2-why-machine-learning-and-advanced-ai-techniques-cannot-be-applied-alone","title":"2. Why Machine Learning and Advanced AI Techniques Cannot Be Applied Alone","text":"<p>While machine learning and other advanced AI techniques have greatly improved data analysis, when it comes down to minute details regarding human language, they always tend to break down. They would fail when it came to the subtlety of language with things like context and meaning-a place where NLP plays an important role.</p> <p>NLP is a branch of AI, committed to the development of techniques and tools that would allow computers to understand, interpret, and produce human language accurately. In contrast to general machine learning methods, NLP is concerned with the specifics of language, its structure and meaning.</p> <p>Machine Learning and other advanced AI techniques have taken the analysis of data further, yet it has not enabled the understanding of human language. Here's why:</p> <ul> <li>Contextual Understanding</li> </ul> <p>They do not capture the essence of the use of words. Human languages are highly context dependent, and this is often beyond the scope of training of ML models.</p> <ul> <li>Semantic Ambiguity</li> </ul> <p>Words take different meanings given different contexts. ML algorithms go haywire when it comes to dealing with this ambiguity, leading to misunderstandings.</p> <ul> <li>Syntax and Structure</li> </ul> <p>The grammar and syntax of human language may be complex and beyond the comprehension capability of ML models. This probably causes mistakes in the meaning obtained.</p> <ul> <li>Pragmatics</li> </ul> <p>This often makes them miss the implied meanings of most messages and social cues, hence the relevance of the responses.</p> <p>As much as MLs and advanced AI are strong, they usually have a lot of problems when it comes to the intricacies of human languages. Actually, NLP is supposed to help in solving such challenges in depth.</p>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#3-why-nlp-is-essential","title":"3. Why NLP Is Essential","text":"<p>NLP enhances various applications, from speech recognition to text analysis, by addressing critical aspects of language processing:</p> <ul> <li> <p>Contextual Understanding: NLP enables computers to grasp the meaning of words and sentences based on context, improving how well they interact with other meanings. This is crucial for effective speech recognition and text analysis.</p> </li> <li> <p>Semantic Analysis: NLP resolves ambiguities such as multiple meanings of words and synonyms, helping machines understand language more accurately.</p> </li> <li> <p>Natural Interaction: NLP facilitates smoother interactions between humans and machines, allowing chatbots and virtual assistants to generate responses that feel more 'human-like.'</p> </li> </ul> <p>These NLP capabilities refine language-based technologies, boosting their performance, accuracy, and contextual awareness in both speech and text. </p>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#nlp-models","title":"NLP Models","text":""},{"location":"NLP-Basics-to-Advanced/Lesson1/#introduction-to-nlp-models","title":"Introduction to NLP Models","text":""},{"location":"NLP-Basics-to-Advanced/Lesson1/#traditional-machine-learning-models","title":"Traditional Machine Learning Models","text":"<ul> <li>Logistic Regression: A simple model used to classify text into categories, such as detecting whether a review is positive or negative.</li> <li>Naive Bayes: A probabilistic model that's effective for text classification tasks, assuming that features (words) are independent of each other (e.g., determining spam vs. non-spam emails).</li> <li>Support Vector Machines (SVM): A model that finds the best line (or hyperplane) to separate different categories of text, useful for tasks like sentiment analysis.</li> <li>Decision Trees: A model that makes decisions by splitting data into branches based on feature values, helping in text classification.</li> <li>K-Nearest Neighbors (KNN): A model that classifies text based on the most common category among its nearest neighbors in feature space.</li> </ul>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#feature-extraction-and-representation-techniques","title":"Feature Extraction and Representation Techniques","text":"<ul> <li>TF-IDF (Term Frequency-Inverse Document Frequency): A method to quantify the importance of words in a document relative to all documents. Useful for converting text into numerical data for machine learning models.</li> <li>Word Embeddings: Techniques like Word2Vec and GloVe convert words into dense, continuous vectors that capture their meanings and relationships. These embeddings are foundational for modern NLP models.</li> <li>Latent Dirichlet Allocation (LDA): A topic modeling technique that discovers hidden topics in a collection of documents by analyzing word distributions.</li> </ul>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#advanced-nlp-models","title":"Advanced NLP Models","text":"<ul> <li>Transformers: Cutting-edge models that use self-attention mechanisms to understand the context of each word in a sentence by considering all words simultaneously. This architecture is the foundation of many advanced NLP models.</li> <li>BERT (Bidirectional Encoder Representations from Transformers): Reads text in both directions (left-to-right and right-to-left) to provide a deep understanding of word meanings and context, improving performance on various NLP tasks.</li> <li>GPT (Generative Pre-trained Transformer): Generates coherent text by predicting the next word in a sequence, useful for applications like content generation and conversational agents.</li> <li>T5 (Text-To-Text Transfer Transformer): Treats every NLP task as a text-to-text problem, making it versatile for tasks like translation, summarization, and question answering.</li> <li>XLNet: Builds on BERT by using a permutation-based approach to better capture context and relationships between words, enhancing performance on several benchmarks.</li> <li>RoBERTa (Robustly optimized BERT approach): An improved version of BERT with better training techniques and data, leading to enhanced performance on a range of NLP tasks.</li> <li>ALBERT (A Lite BERT): A more efficient version of BERT, designed to be smaller and faster while maintaining similar performance.</li> <li>CLIP (Contrastive Language-Image Pretraining): Understands both text and images, enabling applications that require integrating visual and textual information, such as image captioning.</li> <li>DALL-E: Generates images from textual descriptions, combining text and visual creativity for generating unique images based on written prompts.</li> <li>LLaMA (Large Language Model Meta AI): A large-scale language model developed by Meta, designed for a variety of NLP tasks, including text generation and understanding, leveraging transformer architectures.</li> <li>Meta-Learning Models: Includes techniques like MAML (Model-Agnostic Meta-Learning) that allow models to quickly adapt to new tasks with minimal data.</li> </ul>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#4-roadmap-to-nlp","title":"4. Roadmap to NLP","text":"<p>To build a solid foundation in NLP, it's essential to follow a structured learning path. This roadmap outlines the key areas you should focus on, complete with detailed explanations and practical code examples.</p>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#5-cleaning-the-inputtext","title":"5. Cleaning the Input/Text","text":"<p>Before any meaningful analysis can be performed on text data, it's crucial to preprocess and clean the data. This involves several sub-tasks:</p>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#51-cleaning","title":"5.1. Cleaning","text":"<p>Objective: Remove unwanted characters, symbols, and noise from the text to prepare it for further processing.</p> <p>Common Cleaning Steps:</p> <ul> <li>Lowercasing: Convert all text to lowercase to ensure uniformity.</li> <li>Removing Punctuation: Eliminate punctuation marks that may not contribute to the analysis.</li> <li>Removing Numbers: Depending on the application, numbers might be irrelevant.</li> <li>Removing Stop Words: Filter out common words like \"the\", \"is\", \"in\" that do not carry significant meaning.</li> <li>Removing Whitespace: Trim unnecessary spaces.</li> </ul> <p>Code Example:</p> <p><pre><code>import re\nfrom nltk.corpus import stopwords\n\n# Sample text\ntext = \"Hello! Welcome to NLP Basics. Let's clean this text: remove numbers 123, punctuation!!!\"\n\n# Lowercasing\ntext = text.lower()\n\n# Removing punctuation and numbers\ntext = re.sub(r'[^a-z\\s]', '', text)\n\n# Removing stop words\nstop_words = set(stopwords.words('english'))\ntokens = text.split()\nfiltered_tokens = [word for word in tokens if word not in stop_words]\n\n# Rejoining tokens\ncleaned_text = ' '.join(filtered_tokens)\nprint(\"Cleaned Text:\", cleaned_text)\n</code></pre> Expected Output: <pre><code>$ python Lesson1.py:\nCleaned Text: hello welcome nlp basics lets clean text remove numbers punctuation marks\n</code></pre> </p>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#52-tokenization","title":"5.2. Tokenization","text":"<p>Objective: Split the cleaned text into individual units called tokens, which can be words or sentences.</p> <p>Types of Tokenization:</p> <ul> <li>Word Tokenization: Divides text into individual words.</li> <li>Sentence Tokenization: Divides text into individual sentences.</li> </ul> <p><pre><code>import nltk\nfrom nltk.tokenize import word_tokenize, sent_tokenize\n\n# Sample text\ntext = \"Hello! Welcome to NLP Basics. Let's tokenize this text.\"\n\n# Sentence Tokenization\nsentences = sent_tokenize(text)\nprint(\"Sentence Tokenization:\", sentences)\n\n&lt;br/&gt;\n# Word Tokenization\nwords = word_tokenize(text)\nprint(\"Word Tokenization:\", words)\n</code></pre> Expected Output: <pre><code>Sentence Tokenization: ['Hello!', 'Welcome to NLP Basics.', \"Let's tokenize this text.\"]\nWord Tokenization: ['Hello', '!', 'Welcome', 'to', 'NLP', 'Basics', '.', 'Let', \"'s\", 'tokenize', 'this', 'text', '.']\n</code></pre> </p>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#53-stemming","title":"5.3 Stemming","text":"<p>Stemming reduces words to their base or root form. This technique can be useful in reducing word variations for analysis.</p> <p><pre><code>from nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_words = [stemmer.stem(word) for word in tokens]\n\nprint(\"Stemmed Words:\", stemmed_words)\n</code></pre> Expected Output: <pre><code>Stemmed Words: ['hello', 'thi', 'is', 'a', 'sampl', 'text', 'with', 'number', 'and', 'symbol']\n</code></pre> </p>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#54-lemmatization","title":"5.4 Lemmatization","text":"<p>Lemmatization reduces words to their dictionary form, also known as the lemma, considering the context. <pre><code>from nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\nlemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n\nprint(\"Lemmatized Words:\", lemmatized_words)\n</code></pre> Expected Output: <pre><code>Lemmatized Words: ['hello', 'this', 'is', 'a', 'sample', 'text', 'with', 'number', 'and', 'symbol']\n</code></pre> </p>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#6-converting-input-to-vectors","title":"6. Converting Input to Vectors","text":""},{"location":"NLP-Basics-to-Advanced/Lesson1/#-importance-of-vectorization-in-text-classification","title":"- Importance of Vectorization in Text Classification","text":"<ul> <li>Numerical Input: Converts text to numerical data for machine learning algorithms.</li> <li>Feature Encoding: Represents text with vectors, capturing term frequency and context.</li> <li>Dimensionality Reduction: Manages large vocabularies efficiently.</li> </ul> <p>Raw text cannot be processed by algorithms; vectorization enables computational operations on text data.</p> <p> Vectorization is essential for effective text processing, especially in text classification tasks. Here\u2019s a comparison between manual text comparison methods and vectorization.</p>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#-examplesentiment-classification","title":"- Example:Sentiment Classification","text":"<p>Goal: Classify a new movie review as \"positive\" or \"negative\" based on similarity to existing reviews. </p>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#-without-vectorization","title":"-Without Vectorization","text":"<p>Manual string comparison:</p> <p><pre><code># Sample movie reviews and their labels\nreviews = [\n    \"I love this movie\",\n    \"The movie was terrible\",\n    \"Fantastic film!\",\n    \"I hated the film\"\n]\nlabels = ['positive', 'negative', 'positive', 'negative']\n\n# New review to classify\nnew_review = \"The film was great\"\n\n# Manual similarity check\ndef classify_review_manual(new_review, reviews, labels):\n    for i, review in enumerate(reviews):\n        if review.lower() in new_review.lower():\n            return labels[i]\n    return \"unknown\"\n\n# Classify using manual check\npredicted_label_manual = classify_review_manual(new_review, reviews, labels)\nprint(\"Manual Classification:\", predicted_label_manual)\n</code></pre> Expected Output: <pre><code>Manual Classification: unknown\n</code></pre> </p>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#-with-vectorization","title":"- With Vectorization","text":"<p>Using vectorization to handle text:</p> <p><pre><code>import nltk\nfrom nltk.util import ngrams\nfrom collections import Counter\n\n# Function to create bigrams (pairs of consecutive words)\ndef generate_bigrams(text):\n    words = text.lower().split()\n    return [' '.join(gram) for gram in ngrams(words, 2)]\n\n# Function to create bigram features from corpus\ndef create_bigram_features(corpus):\n    all_bigrams = [bigram for text in corpus for bigram in generate_bigrams(text)]\n    bigram_counts = Counter(all_bigrams)\n    return list(bigram_counts.keys())\n\n# Function to convert text into a bigram vector\ndef text_to_vector(text, feature_names):\n    text_bigrams = generate_bigrams(text)\n    text_bigram_counts = Counter(text_bigrams)\n    return [text_bigram_counts.get(bigram, 0) for bigram in feature_names]\n\n# Function to classify new text based on similarity using vectors\ndef classify_text(new_text, training_vectors, training_labels):\n    new_vector = text_to_vector(new_text, feature_names)\n    similarities = [sum(a * b for a, b in zip(new_vector, vec)) for vec in training_vectors]\n    return training_labels[similarities.index(max(similarities))]\n\n# Sample movie reviews and their labels\nreviews = [\n    \"I love this movie\",\n    \"The movie was terrible\",\n    \"Fantastic film!\",\n    \"I hated the film\"\n]\nlabels = ['positive', 'negative', 'positive', 'negative']\n\n# Create feature names from corpus\nfeature_names = create_bigram_features(reviews)\n\n# Convert reviews to vectors\nX = [text_to_vector(text, feature_names) for text in reviews]\n\n# New review to classify\nnew_review = \"The film was great\"\n\n# Classify using vectorization\npredicted_label_vector = classify_text(new_review, X, labels)\nprint(\"Vectorized Classification:\", predicted_label_vector)\n</code></pre> Expected Output: <pre><code>Vectorized Classification: positive\n</code></pre></p> <p>Explanation: Vectorization converts text to numerical vectors, allowing effective similarity measurement. The new review \"The film was great\" is compared against existing review vectors, finding the most similar review (\"Fantastic film!\") and assigning the same label, which is \"positive.\" </p>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#summary","title":"Summary","text":"<p>Without Vectorization: Limited to exact matches; impractical for large or diverse datasets. With Vectorization: Understands context ;scalable and effective for complex text analyses.</p> <p> Once the text is cleaned and tokenized, it must be converted into numerical vectors to be processed by machine learning algorithms. There are two broad approaches for text vectorization:</p>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#61-vectorization-techniques-i-traditional-approaches","title":"6.1 Vectorization Techniques I (Traditional Approaches)","text":""},{"location":"NLP-Basics-to-Advanced/Lesson1/#i-bag-of-words-bow","title":"i. Bag of Words (BoW)","text":"<p>What it does: The Bag of Words model converts text into a matrix of word frequencies or binary occurrence.</p> <p>Code Example:</p> <p><pre><code>from collections import Counter\nfrom nltk.tokenize import word_tokenize\nimport nltk\nnltk.download('punkt')\n\ndef text_to_vector(text):\n    tokens = word_tokenize(text.lower())\n    return Counter(tokens)\n\ntext = \"I love learning NLP from Ankamala tutorials\"\nvector = text_to_vector(text)\nprint(\"Bag of Words Vector:\", vector)\n</code></pre> Expected Output: <pre><code>Bag of Words Vector: Counter({'i': 1, 'love': 1, 'learning': 1, 'nlp': 1, 'from': 1, 'ankamala': 1, 'tutorials': 1})\n</code></pre> </p>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#ii-term-frequency-inverse-document-frequency-tf-idf","title":"ii. Term Frequency-Inverse Document Frequency (TF-IDF)","text":"<p>TF-IDF highlights important words in a document by considering how often they appear across multiple documents.</p> <p>What it does: TF-IDF scores words higher if they are frequent in one document but rare across others.</p> <p>Code Example:</p> <p><pre><code>from sklearn.feature_extraction.text import TfidfVectorizer\n\ncorpus = [\"This is a sample sentence\", \"This is another sentence\"]\ntfidf = TfidfVectorizer()\nresult = tfidf.fit_transform(corpus)\n\nprint(\"Words:\", tfidf.get_feature_names_out())\nprint(\"TF-IDF Scores:\\n\", result.toarray())\n</code></pre> Expected Output: <pre><code>words: ['another', 'is', 'sample', 'sentence', 'this']\nTF-IDF Scores: [[0.  0.38  0.65  0.38  0.65]  [0.65  0.38  0.  0.38  0.65]]\n</code></pre> </p>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#iii-n-grams","title":"iii. N-Grams","text":"<p>An N-gram is a sequence of 'n' words in a sentence. N-grams help capture the context of words better than individual words (unigrams).</p> <p>What it does: N-grams capture word context by analyzing sequences of 'n' words, which improves tasks like text prediction, machine translation, and sentiment analysis.</p> <p>For example, consider the sentence: <code>\"I love learning NLP from Ankamala tutorials.\"</code></p> <ul> <li>Unigram (1-gram): Single words</li> <li> <p><code>[\"I\", \"love\", \"learning\", \"NLP\", \"from\", \"Ankamala\", \"tutorials\"]</code></p> </li> <li> <p>Bigram (2-gram): Pairs of consecutive words</p> </li> <li> <p><code>[(\"I\", \"love\"), (\"love\", \"learning\"), (\"learning\", \"NLP\"), (\"NLP\", \"from\"), (\"from\", \"Ankamala\"), (\"Ankamala\", \"tutorials\")]</code></p> </li> <li> <p>Trigram (3-gram): Triplets of consecutive words</p> </li> <li><code>[(\"I\", \"love\", \"learning\"), (\"love\", \"learning\", \"NLP\"), (\"learning\", \"NLP\", \"from\"), (\"NLP\", \"from\", \"Ankamala\"), (\"from\", \"Ankamala\", \"tutorials\")]</code></li> </ul> <p>Code Example:</p> <p><pre><code>from nltk.util import ngrams\nfrom nltk.tokenize import word_tokenize\nfrom collections import Counter\nimport nltk\nnltk.download('punkt')\n\ndef generate_ngrams(text, n):\n    tokens = word_tokenize(text.lower())\n    return list(ngrams(tokens, n))\n\ndef ngram_to_vector(text, n):\n    n_grams = generate_ngrams(text, n)\n    n_gram_freq = Counter(n_grams)\n    return n_gram_freq\n\n# Example text\ntext = \"I love learning NLP from Ankamala tutorials\"\n\n# Generate bigrams and trigrams\nbigrams = ngram_to_vector(text, 2)\ntrigrams = ngram_to_vector(text, 3)\n\nprint(\"Bigrams Vector:\", bigrams)\nprint(\"Trigrams Vector:\", trigrams)\n</code></pre> Expected Output: <pre><code>Bigrams Vector: Counter({('i', 'love'): 1, ('love', 'learning'): 1, ('learning', 'nlp'): 1, ('nlp', 'from'): 1, ('from', 'ankamala'): 1, ('ankamala', 'tutorials'): 1})\nTrigrams Vector: Counter({('i', 'love', 'learning'): 1, ('love', 'learning', 'nlp'): 1, ('learning', 'nlp', 'from'): 1, ('nlp', 'from', 'ankamala'): 1, ('from', 'ankamala', 'tutorials'): 1})\n</code></pre> </p>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#62-converting-input-to-vector-ii-advanced-approaches","title":"6.2 Converting Input to Vector II (Advanced Approaches)","text":""},{"location":"NLP-Basics-to-Advanced/Lesson1/#i-word2vec-iiaverage-word2vec","title":"i. Word2Vec   \u2003\u2003     ii.Average Word2Vec","text":"<p>i.Word2Vec   - Represents words in a vector space where similar words have closer vector representations.</p>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#why-use-word2vec","title":"Why Use Word2Vec?","text":"<p>Word2Vec helps us understand the meaning of words by representing them as vectors in a space where similar words are close together. Unlike simpler methods like TF-IDF, Word2Vec captures word similarities and relationships better, making it useful for various NLP tasks.</p>"},{"location":"NLP-Basics-to-Advanced/Lesson1/#how-it-works","title":"How it Works","text":"<p>Speech recognition systems convert spoken language into text. Word embeddings like Word2Vec help these systems understand the context and meaning of the words in the text, making the recognition process more accurate.</p> <ul> <li>Introduction to Word Embeddings</li> </ul> <p>Word embeddings are vector representations of words that capture their meanings and relationships. They\u2019re crucial for understanding language in a more nuanced way.</p> <ul> <li>Word2Vec with Gensim</li> </ul> <p>Gensim is a powerful library for working with Word2Vec. Here\u2019s how you can use it:</p> <p>Install Gensim: <pre><code>pip install gensim\n</code></pre> Code Example: <pre><code>from gensim.models import Word2Vec\nfrom gensim.utils import simple_preprocess\n\n# Sample sentences\nsentences = [\n    \"This is a sample sentence.\",\n    \"This is another example of a sentence.\",\n    \"Natural language processing is fun.\",   \n]\n\n# Tokenize and preprocess sentences\npreprocessed_sentences = [simple_preprocess(sentence) for sentence in sentences]\n\n# Train Word2Vec model\nmodel = Word2Vec(sentences=preprocessed_sentences, vector_size=50, window=5, min_count=1, sg=0)\n\n# Get word vectors\nword_vectors = model.wv\n\n# Display vector for a word\nprint(\"Vector for 'sentence':\", word_vectors['sentence'])\n\n# Find similar words\nsimilar_words = word_vectors.most_similar('sentence', topn=3)\nprint(\"Words similar to 'sentence':\", similar_words)\n</code></pre> Expected Output: <pre><code>Vector for 'sentence': [ 0.0132551  -0.0017596  -0.0068934  -0.01502807  0.00064916  0.01293842\n -0.00333229 ....]\n\nWords similar to 'sentence': [('sample', 0.998260498046875), ('another', 0.9979914426803589), ('example', 0.9976629014015198)]\n</code></pre></p>"},{"location":"NLP-Basics-to-Advanced/Lesson2/","title":"Lesson 2","text":""},{"location":"NLP-Basics-to-Advanced/Lesson2/#coming-soon","title":"coming soon...","text":""},{"location":"NLP-Basics-to-Advanced/NLP-Start/","title":"NLP-Basics-to-Advanced","text":""},{"location":"NLP-Basics-to-Advanced/NLP-Start/#nlp-basics-to-advanced","title":"NLP Basics to Advanced","text":"<p>Natural Language Processing, or NLP, is a field of artificial intelligence with a particular concentration on interaction between computers and humans through natural language.  This is a huge-spectrum field that undertakes everything from simple text processing to complex understanding and generation of human languages. Whether one is a complete beginner or an experienced practitioner in this field, the following guide will take one through the key concepts and techniques in NLP, from the most basic ones up to the really advanced. Among other things, our guide shall cover concepts along with code-snippets for quick implementations of acquired knowledge:</p> <p>In this guide, we will cover:</p> <ul> <li>Introduction to NLP: A deeper look at what NLP is and why it is important in today's world.</li> <li>Basic Text Processing:  Understand the cleaning, tokenization, and preprocessing of text data. </li> <li>Part-of-Speech Tagging and Named Entity Recognition: Explore the techniques to identify parts of speech and named entities within a text.</li> <li>Sentiment Analysis: Sentiment Analysis Understand how one may make an estimate of the sentiment within a given text</li> <li>Language Models and Word Embeddings: This module will explore word embeddings, TF-IDF, and some of the pre-trained models like Word2Vec, GloVe, and BERT.</li> <li>Advanced NLP Techniques: Discover deep learning techniques in NLP, such as recurrent neural networks (RNNs), transformers, and sequence-to-sequence models.</li> </ul> <p>By the end, you'll have a very solid grasp of NLP and how to approach a large variety of different real-world NLP tasks.</p>"}]}
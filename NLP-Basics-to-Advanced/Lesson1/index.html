
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://github.com/olafOlaffff/blogs/NLP-Basics-to-Advanced/Lesson1/">
      
      
        <link rel="prev" href="../NLP-Start/">
      
      
        <link rel="next" href="../Lesson2/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.34">
    
    
      
        <title>Lesson 1 - Ankamala Insights - Data and Investigations</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.35f28582.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="peach">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#nlp-basics-to-advanced" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Ankamala Insights - Data and Investigations" class="md-header__button md-logo" aria-label="Ankamala Insights - Data and Investigations" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Ankamala Insights - Data and Investigations
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Lesson 1
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="peach"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6m0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4M7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="deep-blue" data-md-color-accent="peach"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../NLP-Ankamala/" class="md-tabs__link">
          
  
  NLP@Ankamala

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Ankamala Insights - Data and Investigations" class="md-nav__button md-logo" aria-label="Ankamala Insights - Data and Investigations" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Ankamala Insights - Data and Investigations
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  <span class="md-ellipsis">
    NLP@Ankamala
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            NLP@Ankamala
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP-Ankamala/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Overview
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../NLP-Start/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    NLP-Basics-to-Advanced
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Lesson 1
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Lesson 1
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1learning-flow" class="md-nav__link">
    <span class="md-ellipsis">
      1.Learning Flow
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-why-machine-learning-and-advanced-ai-techniques-cannot-be-applied-alone" class="md-nav__link">
    <span class="md-ellipsis">
      2. Why Machine Learning and Advanced AI Techniques Cannot Be Applied Alone
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-why-nlp-is-essential" class="md-nav__link">
    <span class="md-ellipsis">
      3. Why NLP Is Essential
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. Why NLP Is Essential">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#nlp-models" class="md-nav__link">
    <span class="md-ellipsis">
      NLP Models
    </span>
  </a>
  
    <nav class="md-nav" aria-label="NLP Models">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#introduction-to-nlp-models" class="md-nav__link">
    <span class="md-ellipsis">
      Introduction to NLP Models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#traditional-machine-learning-models" class="md-nav__link">
    <span class="md-ellipsis">
      Traditional Machine Learning Models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#feature-extraction-and-representation-techniques" class="md-nav__link">
    <span class="md-ellipsis">
      Feature Extraction and Representation Techniques
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#advanced-nlp-models" class="md-nav__link">
    <span class="md-ellipsis">
      Advanced NLP Models
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-roadmap-to-nlp" class="md-nav__link">
    <span class="md-ellipsis">
      4. Roadmap to NLP
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-cleaning-the-inputtext" class="md-nav__link">
    <span class="md-ellipsis">
      5. Cleaning the Input/Text
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. Cleaning the Input/Text">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#51-cleaning" class="md-nav__link">
    <span class="md-ellipsis">
      5.1. Cleaning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#52-tokenization" class="md-nav__link">
    <span class="md-ellipsis">
      5.2. Tokenization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#53-stemming" class="md-nav__link">
    <span class="md-ellipsis">
      5.3 Stemming
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#54-lemmatization" class="md-nav__link">
    <span class="md-ellipsis">
      5.4 Lemmatization
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-converting-input-to-vectors" class="md-nav__link">
    <span class="md-ellipsis">
      6. Converting Input to Vectors
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6. Converting Input to Vectors">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#-importance-of-vectorization-in-text-classification" class="md-nav__link">
    <span class="md-ellipsis">
      - Importance of Vectorization in Text Classification
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#-examplesentiment-classification" class="md-nav__link">
    <span class="md-ellipsis">
      - Example:Sentiment Classification
    </span>
  </a>
  
    <nav class="md-nav" aria-label="- Example:Sentiment Classification">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#-without-vectorization" class="md-nav__link">
    <span class="md-ellipsis">
      -Without Vectorization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#-with-vectorization" class="md-nav__link">
    <span class="md-ellipsis">
      - With Vectorization
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#summary" class="md-nav__link">
    <span class="md-ellipsis">
      Summary
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#61-vectorization-techniques-i-traditional-approaches" class="md-nav__link">
    <span class="md-ellipsis">
      6.1 Vectorization Techniques I (Traditional Approaches)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6.1 Vectorization Techniques I (Traditional Approaches)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#i-bag-of-words-bow" class="md-nav__link">
    <span class="md-ellipsis">
      i. Bag of Words (BoW)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ii-term-frequency-inverse-document-frequency-tf-idf" class="md-nav__link">
    <span class="md-ellipsis">
      ii. Term Frequency-Inverse Document Frequency (TF-IDF)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#iii-n-grams" class="md-nav__link">
    <span class="md-ellipsis">
      iii. N-Grams
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#62-converting-input-to-vector-ii-advanced-approaches" class="md-nav__link">
    <span class="md-ellipsis">
      6.2 Converting Input to Vector II (Advanced Approaches)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6.2 Converting Input to Vector II (Advanced Approaches)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#i-word2vec-iiaverage-word2vec" class="md-nav__link">
    <span class="md-ellipsis">
      i. Word2Vec &emsp;&emsp; ii.Average Word2Vec
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-use-word2vec" class="md-nav__link">
    <span class="md-ellipsis">
      Why Use Word2Vec?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#how-it-works" class="md-nav__link">
    <span class="md-ellipsis">
      How it Works
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Lesson2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Lesson 2
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="nlp-basics-to-advanced">NLP Basics to Advanced</h1>
<p>Natural Language Processing (NLP) is a critical field within artificial intelligence that bridges the gap between human communication and computer understanding. From enabling voice-activated assistants to powering sophisticated text analytics, NLP plays a pivotal role in making machines understand and respond to human language. This guide will take you through the foundational concepts of NLP, progressing to more advanced techniques, complete with detailed explanations and practical code examples.</p>
<h2 id="1learning-flow">1.Learning Flow</h2>
<ol>
<li><a href="#table-of-content">Learning Flow</a></li>
<li><a href="#why-ML-only-insuffient"> Why Machine Learning and Advanced AI Techniques Cannot Be Applied Alone</a></li>
<li>
<p><a href="#why-NLP-essential">Why NLP Is Essential</a></p>
</li>
<li>
<p><a href="#roadmap-to-nlp">Roadmap to NLP</a></p>
</li>
<li><a href="#cleaning-the-inputtext">Cleaning the Input/Text</a><ul>
<li>5.1 <a href="#cleaning">Cleaning</a></li>
<li>5.2 <a href="#tokenization">Tokenization</a></li>
<li>5.3 <a href="#stemming">Stemming</a></li>
<li>5.4 <a href="#lemmatization">Lemmatization</a></li>
</ul>
</li>
<li><a href="#converting-input-to-vectors">Converting Input to Vectors</a><ul>
<li>6.1 <a href="#vectorization-techniques-i">Vectorization Techniques I</a><ul>
<li>i. <a href="#bag-of-words">Bag of Words</a></li>
<li>ii. <a href="#tf-idf">TF-IDF</a></li>
<li>iii. <a href="#n-grams">N-grams</a></li>
</ul>
</li>
<li>6.2 <a href="#vectorization-techniques-ii">Vectorization Techniques II</a><ul>
<li>i. <a href="#word2vec">Word2Vec</a></li>
<li>ii. <a href="#average-word2vec">Average Word2Vec</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#advanced-vectorization-techniques">Advanced Vectorization Techniques</a><ul>
<li>7.1 <a href="#glove">GloVe</a></li>
<li>7.2 <a href="#fasttext">FastText</a></li>
</ul>
</li>
<li><a href="#part-of-speech-pos-tagging">Part-of-Speech (POS) Tagging</a></li>
<li><a href="#text-classification">Text Classification</a></li>
<li><a href="#named-entity-recognition-ner">Named Entity Recognition (NER)</a></li>
<li><a href="#sentiment-analysis">Sentiment Analysis</a></li>
<li><a href="#language-models-and-transformers">Language Models and Transformers</a></li>
<li><a href="#practical-applications-and-projects">Practical Applications and Projects</a></li>
</ol>
<hr />
<h2 id="2-why-machine-learning-and-advanced-ai-techniques-cannot-be-applied-alone">2. Why Machine Learning and Advanced AI Techniques Cannot Be Applied Alone</h2>
<p>While machine learning and other advanced AI techniques have greatly improved data analysis, when it comes down to minute details regarding human language, they always tend to break down. They would fail when it came to the subtlety of language with things like context and meaning-a place where NLP plays an important role.</p>
<p>NLP is a branch of AI, committed to the development of techniques and tools that would allow computers to understand, interpret, and produce human language accurately. In contrast to general machine learning methods, NLP is concerned with the specifics of language, its structure and meaning.</p>
<p>Machine Learning and other advanced AI techniques have taken the analysis of data further, yet it has not enabled the understanding of human language. Here's why:</p>
<ul>
<li><strong>Contextual Understanding</strong></li>
</ul>
<p>They do not capture the essence of the use of words. Human languages are highly context dependent, and this is often beyond the scope of training of ML models.</p>
<ul>
<li><strong>Semantic Ambiguity</strong></li>
</ul>
<p>Words take different meanings given different contexts. ML algorithms go haywire when it comes to dealing with this ambiguity, leading to misunderstandings.</p>
<ul>
<li><strong>Syntax and Structure</strong></li>
</ul>
<p>The grammar and syntax of human language may be complex and beyond the comprehension capability of ML models. This probably causes mistakes in the meaning obtained.</p>
<ul>
<li><strong>Pragmatics</strong></li>
</ul>
<p>This often makes them miss the implied meanings of most messages and social cues, hence the relevance of the responses.</p>
<p>As much as MLs and advanced AI are strong, they usually have a lot of problems when it comes to the intricacies of human languages. Actually, NLP is supposed to help in solving such challenges in depth.</p>
<hr />
<h2 id="3-why-nlp-is-essential">3. Why NLP Is Essential</h2>
<p>NLP enhances various applications, from speech recognition to text analysis, by addressing critical aspects of language processing:</p>
<ul>
<li>
<p><strong>Contextual Understanding:</strong> NLP enables computers to grasp the meaning of words and sentences based on context, improving how well they interact with other meanings. This is crucial for effective speech recognition and text analysis.</p>
</li>
<li>
<p><strong>Semantic Analysis:</strong> NLP resolves ambiguities such as multiple meanings of words and synonyms, helping machines understand language more accurately.</p>
</li>
<li>
<p><strong>Natural Interaction:</strong> NLP facilitates smoother interactions between humans and machines, allowing chatbots and virtual assistants to generate responses that feel more 'human-like.'</p>
</li>
</ul>
<p>These NLP capabilities refine language-based technologies, boosting their performance, accuracy, and contextual awareness in both speech and text.
<br/></p>
<h3 id="nlp-models">NLP Models</h3>
<h4 id="introduction-to-nlp-models">Introduction to NLP Models</h4>
<h4 id="traditional-machine-learning-models">Traditional Machine Learning Models</h4>
<ul>
<li><strong>Logistic Regression</strong>: A simple model used to classify text into categories, such as detecting whether a review is positive or negative.</li>
<li><strong>Naive Bayes</strong>: A probabilistic model that's effective for text classification tasks, assuming that features (words) are independent of each other (e.g., determining spam vs. non-spam emails).</li>
<li><strong>Support Vector Machines (SVM)</strong>: A model that finds the best line (or hyperplane) to separate different categories of text, useful for tasks like sentiment analysis.</li>
<li><strong>Decision Trees</strong>: A model that makes decisions by splitting data into branches based on feature values, helping in text classification.</li>
<li><strong>K-Nearest Neighbors (KNN)</strong>: A model that classifies text based on the most common category among its nearest neighbors in feature space.</li>
</ul>
<h4 id="feature-extraction-and-representation-techniques">Feature Extraction and Representation Techniques</h4>
<ul>
<li><strong>TF-IDF (Term Frequency-Inverse Document Frequency)</strong>: A method to quantify the importance of words in a document relative to all documents. Useful for converting text into numerical data for machine learning models.</li>
<li><strong>Word Embeddings</strong>: Techniques like Word2Vec and GloVe convert words into dense, continuous vectors that capture their meanings and relationships. These embeddings are foundational for modern NLP models.</li>
<li><strong>Latent Dirichlet Allocation (LDA)</strong>: A topic modeling technique that discovers hidden topics in a collection of documents by analyzing word distributions.</li>
</ul>
<h4 id="advanced-nlp-models">Advanced NLP Models</h4>
<ul>
<li><strong>Transformers</strong>: Cutting-edge models that use self-attention mechanisms to understand the context of each word in a sentence by considering all words simultaneously. This architecture is the foundation of many advanced NLP models.</li>
<li><strong>BERT (Bidirectional Encoder Representations from Transformers)</strong>: Reads text in both directions (left-to-right and right-to-left) to provide a deep understanding of word meanings and context, improving performance on various NLP tasks.</li>
<li><strong>GPT (Generative Pre-trained Transformer)</strong>: Generates coherent text by predicting the next word in a sequence, useful for applications like content generation and conversational agents.</li>
<li><strong>T5 (Text-To-Text Transfer Transformer)</strong>: Treats every NLP task as a text-to-text problem, making it versatile for tasks like translation, summarization, and question answering.</li>
<li><strong>XLNet</strong>: Builds on BERT by using a permutation-based approach to better capture context and relationships between words, enhancing performance on several benchmarks.</li>
<li><strong>RoBERTa (Robustly optimized BERT approach)</strong>: An improved version of BERT with better training techniques and data, leading to enhanced performance on a range of NLP tasks.</li>
<li><strong>ALBERT (A Lite BERT)</strong>: A more efficient version of BERT, designed to be smaller and faster while maintaining similar performance.</li>
<li><strong>CLIP (Contrastive Language-Image Pretraining)</strong>: Understands both text and images, enabling applications that require integrating visual and textual information, such as image captioning.</li>
<li><strong>DALL-E</strong>: Generates images from textual descriptions, combining text and visual creativity for generating unique images based on written prompts.</li>
<li><strong>LLaMA (Large Language Model Meta AI)</strong>: A large-scale language model developed by Meta, designed for a variety of NLP tasks, including text generation and understanding, leveraging transformer architectures.</li>
<li><strong>Meta-Learning Models</strong>: Includes techniques like MAML (Model-Agnostic Meta-Learning) that allow models to quickly adapt to new tasks with minimal data.</li>
</ul>
<h2 id="4-roadmap-to-nlp">4. Roadmap to NLP</h2>
<p>To build a solid foundation in NLP, it's essential to follow a structured learning path. This roadmap outlines the key areas you should focus on, complete with detailed explanations and practical code examples.</p>
<h2 id="5-cleaning-the-inputtext">5. Cleaning the Input/Text</h2>
<p>Before any meaningful analysis can be performed on text data, it's crucial to preprocess and clean the data. This involves several sub-tasks:</p>
<h4 id="51-cleaning">5.1. Cleaning</h4>
<p><strong>Objective:</strong> Remove unwanted characters, symbols, and noise from the text to prepare it for further processing.</p>
<p><strong>Common Cleaning Steps:</strong></p>
<ul>
<li><strong>Lowercasing:</strong> Convert all text to lowercase to ensure uniformity.</li>
<li><strong>Removing Punctuation:</strong> Eliminate punctuation marks that may not contribute to the analysis.</li>
<li><strong>Removing Numbers:</strong> Depending on the application, numbers might be irrelevant.</li>
<li><strong>Removing Stop Words:</strong> Filter out common words like "the", "is", "in" that do not carry significant meaning.</li>
<li><strong>Removing Whitespace:</strong> Trim unnecessary spaces.</li>
</ul>
<p><strong>Code Example:</strong></p>
<p><div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="kn">import</span> <span class="nn">re</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a><span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a><span class="c1"># Sample text</span>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a><span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;Hello! Welcome to NLP Basics. Let&#39;s clean this text: remove numbers 123, punctuation!!!&quot;</span>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a><span class="c1"># Lowercasing</span>
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a><span class="n">text</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a>
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a><span class="c1"># Removing punctuation and numbers</span>
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a><span class="n">text</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;[^a-z\s]&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a>
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a><span class="c1"># Removing stop words</span>
<a id="__codelineno-0-14" name="__codelineno-0-14" href="#__codelineno-0-14"></a><span class="n">stop_words</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s1">&#39;english&#39;</span><span class="p">))</span>
<a id="__codelineno-0-15" name="__codelineno-0-15" href="#__codelineno-0-15"></a><span class="n">tokens</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
<a id="__codelineno-0-16" name="__codelineno-0-16" href="#__codelineno-0-16"></a><span class="n">filtered_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">tokens</span> <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stop_words</span><span class="p">]</span>
<a id="__codelineno-0-17" name="__codelineno-0-17" href="#__codelineno-0-17"></a>
<a id="__codelineno-0-18" name="__codelineno-0-18" href="#__codelineno-0-18"></a><span class="c1"># Rejoining tokens</span>
<a id="__codelineno-0-19" name="__codelineno-0-19" href="#__codelineno-0-19"></a><span class="n">cleaned_text</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">filtered_tokens</span><span class="p">)</span>
<a id="__codelineno-0-20" name="__codelineno-0-20" href="#__codelineno-0-20"></a><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cleaned Text:&quot;</span><span class="p">,</span> <span class="n">cleaned_text</span><span class="p">)</span>
</code></pre></div>
<strong>Expected Output:</strong>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a>$ python Lesson1.py:
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a>Cleaned Text: hello welcome nlp basics lets clean text remove numbers punctuation marks
</code></pre></div>
<br/></p>
<h4 id="52-tokenization">5.2. Tokenization</h4>
<p><strong>Objective:</strong> Split the cleaned text into individual units called tokens, which can be words or sentences.</p>
<p><strong>Types of Tokenization:</strong></p>
<ul>
<li>Word Tokenization: Divides text into individual words.</li>
<li>Sentence Tokenization: Divides text into individual sentences.</li>
</ul>
<p><div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a><span class="kn">import</span> <span class="nn">nltk</span>
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a><span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">word_tokenize</span><span class="p">,</span> <span class="n">sent_tokenize</span>
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a>
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a><span class="c1"># Sample text</span>
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a><span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;Hello! Welcome to NLP Basics. Let&#39;s tokenize this text.&quot;</span>
<a id="__codelineno-2-6" name="__codelineno-2-6" href="#__codelineno-2-6"></a>
<a id="__codelineno-2-7" name="__codelineno-2-7" href="#__codelineno-2-7"></a><span class="c1"># Sentence Tokenization</span>
<a id="__codelineno-2-8" name="__codelineno-2-8" href="#__codelineno-2-8"></a><span class="n">sentences</span> <span class="o">=</span> <span class="n">sent_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<a id="__codelineno-2-9" name="__codelineno-2-9" href="#__codelineno-2-9"></a><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sentence Tokenization:&quot;</span><span class="p">,</span> <span class="n">sentences</span><span class="p">)</span>
<a id="__codelineno-2-10" name="__codelineno-2-10" href="#__codelineno-2-10"></a>
<a id="__codelineno-2-11" name="__codelineno-2-11" href="#__codelineno-2-11"></a><span class="o">&lt;</span><span class="n">br</span><span class="o">/&gt;</span>
<a id="__codelineno-2-12" name="__codelineno-2-12" href="#__codelineno-2-12"></a><span class="c1"># Word Tokenization</span>
<a id="__codelineno-2-13" name="__codelineno-2-13" href="#__codelineno-2-13"></a><span class="n">words</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<a id="__codelineno-2-14" name="__codelineno-2-14" href="#__codelineno-2-14"></a><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Word Tokenization:&quot;</span><span class="p">,</span> <span class="n">words</span><span class="p">)</span>
</code></pre></div>
<strong>Expected Output:</strong>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a>Sentence Tokenization: [&#39;Hello!&#39;, &#39;Welcome to NLP Basics.&#39;, &quot;Let&#39;s tokenize this text.&quot;]
<a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a>Word Tokenization: [&#39;Hello&#39;, &#39;!&#39;, &#39;Welcome&#39;, &#39;to&#39;, &#39;NLP&#39;, &#39;Basics&#39;, &#39;.&#39;, &#39;Let&#39;, &quot;&#39;s&quot;, &#39;tokenize&#39;, &#39;this&#39;, &#39;text&#39;, &#39;.&#39;]
</code></pre></div>
<br/></p>
<h4 id="53-stemming">5.3 Stemming</h4>
<p>Stemming reduces words to their base or root form. This technique can be useful in reducing word variations for analysis.</p>
<p><div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a><span class="kn">from</span> <span class="nn">nltk.stem</span> <span class="kn">import</span> <span class="n">PorterStemmer</span>
<a id="__codelineno-4-2" name="__codelineno-4-2" href="#__codelineno-4-2"></a>
<a id="__codelineno-4-3" name="__codelineno-4-3" href="#__codelineno-4-3"></a><span class="n">stemmer</span> <span class="o">=</span> <span class="n">PorterStemmer</span><span class="p">()</span>
<a id="__codelineno-4-4" name="__codelineno-4-4" href="#__codelineno-4-4"></a><span class="n">stemmed_words</span> <span class="o">=</span> <span class="p">[</span><span class="n">stemmer</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>
<a id="__codelineno-4-5" name="__codelineno-4-5" href="#__codelineno-4-5"></a>
<a id="__codelineno-4-6" name="__codelineno-4-6" href="#__codelineno-4-6"></a><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Stemmed Words:&quot;</span><span class="p">,</span> <span class="n">stemmed_words</span><span class="p">)</span>
</code></pre></div>
<strong>Expected Output:</strong>
<div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a>Stemmed Words: [&#39;hello&#39;, &#39;thi&#39;, &#39;is&#39;, &#39;a&#39;, &#39;sampl&#39;, &#39;text&#39;, &#39;with&#39;, &#39;number&#39;, &#39;and&#39;, &#39;symbol&#39;]
</code></pre></div>
<br/></p>
<h4 id="54-lemmatization">5.4 Lemmatization</h4>
<p>Lemmatization reduces words to their dictionary form, also known as the lemma, considering the context.
<div class="highlight"><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a><span class="kn">from</span> <span class="nn">nltk.stem</span> <span class="kn">import</span> <span class="n">WordNetLemmatizer</span>
<a id="__codelineno-6-2" name="__codelineno-6-2" href="#__codelineno-6-2"></a>
<a id="__codelineno-6-3" name="__codelineno-6-3" href="#__codelineno-6-3"></a><span class="n">lemmatizer</span> <span class="o">=</span> <span class="n">WordNetLemmatizer</span><span class="p">()</span>
<a id="__codelineno-6-4" name="__codelineno-6-4" href="#__codelineno-6-4"></a><span class="n">lemmatized_words</span> <span class="o">=</span> <span class="p">[</span><span class="n">lemmatizer</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>
<a id="__codelineno-6-5" name="__codelineno-6-5" href="#__codelineno-6-5"></a>
<a id="__codelineno-6-6" name="__codelineno-6-6" href="#__codelineno-6-6"></a><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Lemmatized Words:&quot;</span><span class="p">,</span> <span class="n">lemmatized_words</span><span class="p">)</span>
</code></pre></div>
<strong>Expected Output:</strong>
<div class="highlight"><pre><span></span><code><a id="__codelineno-7-1" name="__codelineno-7-1" href="#__codelineno-7-1"></a>Lemmatized Words: [&#39;hello&#39;, &#39;this&#39;, &#39;is&#39;, &#39;a&#39;, &#39;sample&#39;, &#39;text&#39;, &#39;with&#39;, &#39;number&#39;, &#39;and&#39;, &#39;symbol&#39;]
</code></pre></div>
<br/></p>
<h2 id="6-converting-input-to-vectors">6. Converting Input to Vectors</h2>
<h3 id="-importance-of-vectorization-in-text-classification">- Importance of Vectorization in Text Classification</h3>
<ul>
<li><strong>Numerical Input:</strong> Converts text to numerical data for machine learning algorithms.</li>
<li><strong>Feature Encoding:</strong> Represents text with vectors, capturing term frequency and context.</li>
<li><strong>Dimensionality Reduction:</strong> Manages large vocabularies efficiently.</li>
</ul>
<p>Raw text cannot be processed by algorithms; vectorization enables computational operations on text data.</p>
<p><br/>
Vectorization is essential for effective text processing, especially in text classification tasks. Here’s a comparison between manual text comparison methods and vectorization.</p>
<h3 id="-examplesentiment-classification">- Example:Sentiment Classification</h3>
<p><strong>Goal</strong>: Classify a new movie review as "positive" or "negative" based on similarity to existing reviews.
<br/></p>
<h4 id="-without-vectorization">-Without Vectorization</h4>
<p>Manual string comparison:</p>
<p><div class="highlight"><pre><span></span><code><a id="__codelineno-8-1" name="__codelineno-8-1" href="#__codelineno-8-1"></a><span class="c1"># Sample movie reviews and their labels</span>
<a id="__codelineno-8-2" name="__codelineno-8-2" href="#__codelineno-8-2"></a><span class="n">reviews</span> <span class="o">=</span> <span class="p">[</span>
<a id="__codelineno-8-3" name="__codelineno-8-3" href="#__codelineno-8-3"></a>    <span class="s2">&quot;I love this movie&quot;</span><span class="p">,</span>
<a id="__codelineno-8-4" name="__codelineno-8-4" href="#__codelineno-8-4"></a>    <span class="s2">&quot;The movie was terrible&quot;</span><span class="p">,</span>
<a id="__codelineno-8-5" name="__codelineno-8-5" href="#__codelineno-8-5"></a>    <span class="s2">&quot;Fantastic film!&quot;</span><span class="p">,</span>
<a id="__codelineno-8-6" name="__codelineno-8-6" href="#__codelineno-8-6"></a>    <span class="s2">&quot;I hated the film&quot;</span>
<a id="__codelineno-8-7" name="__codelineno-8-7" href="#__codelineno-8-7"></a><span class="p">]</span>
<a id="__codelineno-8-8" name="__codelineno-8-8" href="#__codelineno-8-8"></a><span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;positive&#39;</span><span class="p">,</span> <span class="s1">&#39;negative&#39;</span><span class="p">,</span> <span class="s1">&#39;positive&#39;</span><span class="p">,</span> <span class="s1">&#39;negative&#39;</span><span class="p">]</span>
<a id="__codelineno-8-9" name="__codelineno-8-9" href="#__codelineno-8-9"></a>
<a id="__codelineno-8-10" name="__codelineno-8-10" href="#__codelineno-8-10"></a><span class="c1"># New review to classify</span>
<a id="__codelineno-8-11" name="__codelineno-8-11" href="#__codelineno-8-11"></a><span class="n">new_review</span> <span class="o">=</span> <span class="s2">&quot;The film was great&quot;</span>
<a id="__codelineno-8-12" name="__codelineno-8-12" href="#__codelineno-8-12"></a>
<a id="__codelineno-8-13" name="__codelineno-8-13" href="#__codelineno-8-13"></a><span class="c1"># Manual similarity check</span>
<a id="__codelineno-8-14" name="__codelineno-8-14" href="#__codelineno-8-14"></a><span class="k">def</span> <span class="nf">classify_review_manual</span><span class="p">(</span><span class="n">new_review</span><span class="p">,</span> <span class="n">reviews</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
<a id="__codelineno-8-15" name="__codelineno-8-15" href="#__codelineno-8-15"></a>    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">review</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">reviews</span><span class="p">):</span>
<a id="__codelineno-8-16" name="__codelineno-8-16" href="#__codelineno-8-16"></a>        <span class="k">if</span> <span class="n">review</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">in</span> <span class="n">new_review</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
<a id="__codelineno-8-17" name="__codelineno-8-17" href="#__codelineno-8-17"></a>            <span class="k">return</span> <span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<a id="__codelineno-8-18" name="__codelineno-8-18" href="#__codelineno-8-18"></a>    <span class="k">return</span> <span class="s2">&quot;unknown&quot;</span>
<a id="__codelineno-8-19" name="__codelineno-8-19" href="#__codelineno-8-19"></a>
<a id="__codelineno-8-20" name="__codelineno-8-20" href="#__codelineno-8-20"></a><span class="c1"># Classify using manual check</span>
<a id="__codelineno-8-21" name="__codelineno-8-21" href="#__codelineno-8-21"></a><span class="n">predicted_label_manual</span> <span class="o">=</span> <span class="n">classify_review_manual</span><span class="p">(</span><span class="n">new_review</span><span class="p">,</span> <span class="n">reviews</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<a id="__codelineno-8-22" name="__codelineno-8-22" href="#__codelineno-8-22"></a><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Manual Classification:&quot;</span><span class="p">,</span> <span class="n">predicted_label_manual</span><span class="p">)</span>
</code></pre></div>
<strong>Expected Output:</strong>
<div class="highlight"><pre><span></span><code><a id="__codelineno-9-1" name="__codelineno-9-1" href="#__codelineno-9-1"></a>Manual Classification: unknown
</code></pre></div>
<br/></p>
<h4 id="-with-vectorization">- With Vectorization</h4>
<p>Using vectorization to handle text:</p>
<p><div class="highlight"><pre><span></span><code><a id="__codelineno-10-1" name="__codelineno-10-1" href="#__codelineno-10-1"></a><span class="kn">import</span> <span class="nn">nltk</span>
<a id="__codelineno-10-2" name="__codelineno-10-2" href="#__codelineno-10-2"></a><span class="kn">from</span> <span class="nn">nltk.util</span> <span class="kn">import</span> <span class="n">ngrams</span>
<a id="__codelineno-10-3" name="__codelineno-10-3" href="#__codelineno-10-3"></a><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>
<a id="__codelineno-10-4" name="__codelineno-10-4" href="#__codelineno-10-4"></a>
<a id="__codelineno-10-5" name="__codelineno-10-5" href="#__codelineno-10-5"></a><span class="c1"># Function to create bigrams (pairs of consecutive words)</span>
<a id="__codelineno-10-6" name="__codelineno-10-6" href="#__codelineno-10-6"></a><span class="k">def</span> <span class="nf">generate_bigrams</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
<a id="__codelineno-10-7" name="__codelineno-10-7" href="#__codelineno-10-7"></a>    <span class="n">words</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
<a id="__codelineno-10-8" name="__codelineno-10-8" href="#__codelineno-10-8"></a>    <span class="k">return</span> <span class="p">[</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">gram</span><span class="p">)</span> <span class="k">for</span> <span class="n">gram</span> <span class="ow">in</span> <span class="n">ngrams</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="mi">2</span><span class="p">)]</span>
<a id="__codelineno-10-9" name="__codelineno-10-9" href="#__codelineno-10-9"></a>
<a id="__codelineno-10-10" name="__codelineno-10-10" href="#__codelineno-10-10"></a><span class="c1"># Function to create bigram features from corpus</span>
<a id="__codelineno-10-11" name="__codelineno-10-11" href="#__codelineno-10-11"></a><span class="k">def</span> <span class="nf">create_bigram_features</span><span class="p">(</span><span class="n">corpus</span><span class="p">):</span>
<a id="__codelineno-10-12" name="__codelineno-10-12" href="#__codelineno-10-12"></a>    <span class="n">all_bigrams</span> <span class="o">=</span> <span class="p">[</span><span class="n">bigram</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">corpus</span> <span class="k">for</span> <span class="n">bigram</span> <span class="ow">in</span> <span class="n">generate_bigrams</span><span class="p">(</span><span class="n">text</span><span class="p">)]</span>
<a id="__codelineno-10-13" name="__codelineno-10-13" href="#__codelineno-10-13"></a>    <span class="n">bigram_counts</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">all_bigrams</span><span class="p">)</span>
<a id="__codelineno-10-14" name="__codelineno-10-14" href="#__codelineno-10-14"></a>    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">bigram_counts</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<a id="__codelineno-10-15" name="__codelineno-10-15" href="#__codelineno-10-15"></a>
<a id="__codelineno-10-16" name="__codelineno-10-16" href="#__codelineno-10-16"></a><span class="c1"># Function to convert text into a bigram vector</span>
<a id="__codelineno-10-17" name="__codelineno-10-17" href="#__codelineno-10-17"></a><span class="k">def</span> <span class="nf">text_to_vector</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">feature_names</span><span class="p">):</span>
<a id="__codelineno-10-18" name="__codelineno-10-18" href="#__codelineno-10-18"></a>    <span class="n">text_bigrams</span> <span class="o">=</span> <span class="n">generate_bigrams</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<a id="__codelineno-10-19" name="__codelineno-10-19" href="#__codelineno-10-19"></a>    <span class="n">text_bigram_counts</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">text_bigrams</span><span class="p">)</span>
<a id="__codelineno-10-20" name="__codelineno-10-20" href="#__codelineno-10-20"></a>    <span class="k">return</span> <span class="p">[</span><span class="n">text_bigram_counts</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">bigram</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">bigram</span> <span class="ow">in</span> <span class="n">feature_names</span><span class="p">]</span>
<a id="__codelineno-10-21" name="__codelineno-10-21" href="#__codelineno-10-21"></a>
<a id="__codelineno-10-22" name="__codelineno-10-22" href="#__codelineno-10-22"></a><span class="c1"># Function to classify new text based on similarity using vectors</span>
<a id="__codelineno-10-23" name="__codelineno-10-23" href="#__codelineno-10-23"></a><span class="k">def</span> <span class="nf">classify_text</span><span class="p">(</span><span class="n">new_text</span><span class="p">,</span> <span class="n">training_vectors</span><span class="p">,</span> <span class="n">training_labels</span><span class="p">):</span>
<a id="__codelineno-10-24" name="__codelineno-10-24" href="#__codelineno-10-24"></a>    <span class="n">new_vector</span> <span class="o">=</span> <span class="n">text_to_vector</span><span class="p">(</span><span class="n">new_text</span><span class="p">,</span> <span class="n">feature_names</span><span class="p">)</span>
<a id="__codelineno-10-25" name="__codelineno-10-25" href="#__codelineno-10-25"></a>    <span class="n">similarities</span> <span class="o">=</span> <span class="p">[</span><span class="nb">sum</span><span class="p">(</span><span class="n">a</span> <span class="o">*</span> <span class="n">b</span> <span class="k">for</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">new_vector</span><span class="p">,</span> <span class="n">vec</span><span class="p">))</span> <span class="k">for</span> <span class="n">vec</span> <span class="ow">in</span> <span class="n">training_vectors</span><span class="p">]</span>
<a id="__codelineno-10-26" name="__codelineno-10-26" href="#__codelineno-10-26"></a>    <span class="k">return</span> <span class="n">training_labels</span><span class="p">[</span><span class="n">similarities</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">similarities</span><span class="p">))]</span>
<a id="__codelineno-10-27" name="__codelineno-10-27" href="#__codelineno-10-27"></a>
<a id="__codelineno-10-28" name="__codelineno-10-28" href="#__codelineno-10-28"></a><span class="c1"># Sample movie reviews and their labels</span>
<a id="__codelineno-10-29" name="__codelineno-10-29" href="#__codelineno-10-29"></a><span class="n">reviews</span> <span class="o">=</span> <span class="p">[</span>
<a id="__codelineno-10-30" name="__codelineno-10-30" href="#__codelineno-10-30"></a>    <span class="s2">&quot;I love this movie&quot;</span><span class="p">,</span>
<a id="__codelineno-10-31" name="__codelineno-10-31" href="#__codelineno-10-31"></a>    <span class="s2">&quot;The movie was terrible&quot;</span><span class="p">,</span>
<a id="__codelineno-10-32" name="__codelineno-10-32" href="#__codelineno-10-32"></a>    <span class="s2">&quot;Fantastic film!&quot;</span><span class="p">,</span>
<a id="__codelineno-10-33" name="__codelineno-10-33" href="#__codelineno-10-33"></a>    <span class="s2">&quot;I hated the film&quot;</span>
<a id="__codelineno-10-34" name="__codelineno-10-34" href="#__codelineno-10-34"></a><span class="p">]</span>
<a id="__codelineno-10-35" name="__codelineno-10-35" href="#__codelineno-10-35"></a><span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;positive&#39;</span><span class="p">,</span> <span class="s1">&#39;negative&#39;</span><span class="p">,</span> <span class="s1">&#39;positive&#39;</span><span class="p">,</span> <span class="s1">&#39;negative&#39;</span><span class="p">]</span>
<a id="__codelineno-10-36" name="__codelineno-10-36" href="#__codelineno-10-36"></a>
<a id="__codelineno-10-37" name="__codelineno-10-37" href="#__codelineno-10-37"></a><span class="c1"># Create feature names from corpus</span>
<a id="__codelineno-10-38" name="__codelineno-10-38" href="#__codelineno-10-38"></a><span class="n">feature_names</span> <span class="o">=</span> <span class="n">create_bigram_features</span><span class="p">(</span><span class="n">reviews</span><span class="p">)</span>
<a id="__codelineno-10-39" name="__codelineno-10-39" href="#__codelineno-10-39"></a>
<a id="__codelineno-10-40" name="__codelineno-10-40" href="#__codelineno-10-40"></a><span class="c1"># Convert reviews to vectors</span>
<a id="__codelineno-10-41" name="__codelineno-10-41" href="#__codelineno-10-41"></a><span class="n">X</span> <span class="o">=</span> <span class="p">[</span><span class="n">text_to_vector</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">feature_names</span><span class="p">)</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">reviews</span><span class="p">]</span>
<a id="__codelineno-10-42" name="__codelineno-10-42" href="#__codelineno-10-42"></a>
<a id="__codelineno-10-43" name="__codelineno-10-43" href="#__codelineno-10-43"></a><span class="c1"># New review to classify</span>
<a id="__codelineno-10-44" name="__codelineno-10-44" href="#__codelineno-10-44"></a><span class="n">new_review</span> <span class="o">=</span> <span class="s2">&quot;The film was great&quot;</span>
<a id="__codelineno-10-45" name="__codelineno-10-45" href="#__codelineno-10-45"></a>
<a id="__codelineno-10-46" name="__codelineno-10-46" href="#__codelineno-10-46"></a><span class="c1"># Classify using vectorization</span>
<a id="__codelineno-10-47" name="__codelineno-10-47" href="#__codelineno-10-47"></a><span class="n">predicted_label_vector</span> <span class="o">=</span> <span class="n">classify_text</span><span class="p">(</span><span class="n">new_review</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<a id="__codelineno-10-48" name="__codelineno-10-48" href="#__codelineno-10-48"></a><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vectorized Classification:&quot;</span><span class="p">,</span> <span class="n">predicted_label_vector</span><span class="p">)</span>
</code></pre></div>
<strong>Expected Output:</strong>
<div class="highlight"><pre><span></span><code><a id="__codelineno-11-1" name="__codelineno-11-1" href="#__codelineno-11-1"></a>Vectorized Classification: positive
</code></pre></div></p>
<p><strong>Explanation:</strong> Vectorization converts text to numerical vectors, allowing effective similarity measurement. The new review "The film was great" is compared against existing review vectors, finding the most similar review ("Fantastic film!") and assigning the same label, which is "positive."
<br/></p>
<h3 id="summary">Summary</h3>
<p><strong>Without Vectorization:</strong> Limited to exact matches; impractical for large or diverse datasets.
<br /><strong>With Vectorization:</strong> Understands context ;scalable and effective for complex text analyses.</p>
<p><br/>
Once the text is cleaned and tokenized, it must be converted into numerical vectors to be processed by machine learning algorithms. There are two broad approaches for text vectorization:</p>
<h3 id="61-vectorization-techniques-i-traditional-approaches">6.1 Vectorization Techniques I (Traditional Approaches)</h3>
<h4 id="i-bag-of-words-bow">i. Bag of Words (BoW)</h4>
<p><strong>What it does:</strong> The Bag of Words model converts text into a matrix of word frequencies or binary occurrence.</p>
<p><strong>Code Example:</strong></p>
<p><div class="highlight"><pre><span></span><code><a id="__codelineno-12-1" name="__codelineno-12-1" href="#__codelineno-12-1"></a><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>
<a id="__codelineno-12-2" name="__codelineno-12-2" href="#__codelineno-12-2"></a><span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">word_tokenize</span>
<a id="__codelineno-12-3" name="__codelineno-12-3" href="#__codelineno-12-3"></a><span class="kn">import</span> <span class="nn">nltk</span>
<a id="__codelineno-12-4" name="__codelineno-12-4" href="#__codelineno-12-4"></a><span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;punkt&#39;</span><span class="p">)</span>
<a id="__codelineno-12-5" name="__codelineno-12-5" href="#__codelineno-12-5"></a>
<a id="__codelineno-12-6" name="__codelineno-12-6" href="#__codelineno-12-6"></a><span class="k">def</span> <span class="nf">text_to_vector</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
<a id="__codelineno-12-7" name="__codelineno-12-7" href="#__codelineno-12-7"></a>    <span class="n">tokens</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span>
<a id="__codelineno-12-8" name="__codelineno-12-8" href="#__codelineno-12-8"></a>    <span class="k">return</span> <span class="n">Counter</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
<a id="__codelineno-12-9" name="__codelineno-12-9" href="#__codelineno-12-9"></a>
<a id="__codelineno-12-10" name="__codelineno-12-10" href="#__codelineno-12-10"></a><span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;I love learning NLP from Ankamala tutorials&quot;</span>
<a id="__codelineno-12-11" name="__codelineno-12-11" href="#__codelineno-12-11"></a><span class="n">vector</span> <span class="o">=</span> <span class="n">text_to_vector</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<a id="__codelineno-12-12" name="__codelineno-12-12" href="#__codelineno-12-12"></a><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Bag of Words Vector:&quot;</span><span class="p">,</span> <span class="n">vector</span><span class="p">)</span>
</code></pre></div>
<strong>Expected Output:</strong>
<div class="highlight"><pre><span></span><code><a id="__codelineno-13-1" name="__codelineno-13-1" href="#__codelineno-13-1"></a>Bag of Words Vector: Counter({&#39;i&#39;: 1, &#39;love&#39;: 1, &#39;learning&#39;: 1, &#39;nlp&#39;: 1, &#39;from&#39;: 1, &#39;ankamala&#39;: 1, &#39;tutorials&#39;: 1})
</code></pre></div>
<br/></p>
<h4 id="ii-term-frequency-inverse-document-frequency-tf-idf">ii. Term Frequency-Inverse Document Frequency (TF-IDF)</h4>
<p>TF-IDF highlights important words in a document by considering how often they appear across multiple documents.</p>
<p><strong>What it does:</strong> TF-IDF scores words higher if they are frequent in one document but rare across others.</p>
<p><strong>Code Example:</strong></p>
<p><div class="highlight"><pre><span></span><code><a id="__codelineno-14-1" name="__codelineno-14-1" href="#__codelineno-14-1"></a><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>
<a id="__codelineno-14-2" name="__codelineno-14-2" href="#__codelineno-14-2"></a>
<a id="__codelineno-14-3" name="__codelineno-14-3" href="#__codelineno-14-3"></a><span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;This is a sample sentence&quot;</span><span class="p">,</span> <span class="s2">&quot;This is another sentence&quot;</span><span class="p">]</span>
<a id="__codelineno-14-4" name="__codelineno-14-4" href="#__codelineno-14-4"></a><span class="n">tfidf</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">()</span>
<a id="__codelineno-14-5" name="__codelineno-14-5" href="#__codelineno-14-5"></a><span class="n">result</span> <span class="o">=</span> <span class="n">tfidf</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<a id="__codelineno-14-6" name="__codelineno-14-6" href="#__codelineno-14-6"></a>
<a id="__codelineno-14-7" name="__codelineno-14-7" href="#__codelineno-14-7"></a><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Words:&quot;</span><span class="p">,</span> <span class="n">tfidf</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">())</span>
<a id="__codelineno-14-8" name="__codelineno-14-8" href="#__codelineno-14-8"></a><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;TF-IDF Scores:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">result</span><span class="o">.</span><span class="n">toarray</span><span class="p">())</span>
</code></pre></div>
<strong>Expected Output:</strong>
<div class="highlight"><pre><span></span><code><a id="__codelineno-15-1" name="__codelineno-15-1" href="#__codelineno-15-1"></a>words: [&#39;another&#39;, &#39;is&#39;, &#39;sample&#39;, &#39;sentence&#39;, &#39;this&#39;]
<a id="__codelineno-15-2" name="__codelineno-15-2" href="#__codelineno-15-2"></a>TF-IDF Scores: [[0.  0.38  0.65  0.38  0.65]  [0.65  0.38  0.  0.38  0.65]]
</code></pre></div>
<br/></p>
<h4 id="iii-n-grams">iii. N-Grams</h4>
<p>An N-gram is a sequence of 'n' words in a sentence. N-grams help capture the context of words better than individual words (unigrams).</p>
<p><strong>What it does:</strong> N-grams capture word context by analyzing sequences of 'n' words, which improves tasks like text prediction, machine translation, and sentiment analysis.</p>
<p>For example, consider the sentence: <code>"I love learning NLP from Ankamala tutorials."</code></p>
<ul>
<li><strong>Unigram (1-gram)</strong>: Single words</li>
<li>
<p><code>["I", "love", "learning", "NLP", "from", "Ankamala", "tutorials"]</code></p>
</li>
<li>
<p><strong>Bigram (2-gram)</strong>: Pairs of consecutive words</p>
</li>
<li>
<p><code>[("I", "love"), ("love", "learning"), ("learning", "NLP"), ("NLP", "from"), ("from", "Ankamala"), ("Ankamala", "tutorials")]</code></p>
</li>
<li>
<p><strong>Trigram (3-gram)</strong>: Triplets of consecutive words</p>
</li>
<li><code>[("I", "love", "learning"), ("love", "learning", "NLP"), ("learning", "NLP", "from"), ("NLP", "from", "Ankamala"), ("from", "Ankamala", "tutorials")]</code></li>
</ul>
<p><strong>Code Example:</strong></p>
<p><div class="highlight"><pre><span></span><code><a id="__codelineno-16-1" name="__codelineno-16-1" href="#__codelineno-16-1"></a><span class="kn">from</span> <span class="nn">nltk.util</span> <span class="kn">import</span> <span class="n">ngrams</span>
<a id="__codelineno-16-2" name="__codelineno-16-2" href="#__codelineno-16-2"></a><span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">word_tokenize</span>
<a id="__codelineno-16-3" name="__codelineno-16-3" href="#__codelineno-16-3"></a><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>
<a id="__codelineno-16-4" name="__codelineno-16-4" href="#__codelineno-16-4"></a><span class="kn">import</span> <span class="nn">nltk</span>
<a id="__codelineno-16-5" name="__codelineno-16-5" href="#__codelineno-16-5"></a><span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;punkt&#39;</span><span class="p">)</span>
<a id="__codelineno-16-6" name="__codelineno-16-6" href="#__codelineno-16-6"></a>
<a id="__codelineno-16-7" name="__codelineno-16-7" href="#__codelineno-16-7"></a><span class="k">def</span> <span class="nf">generate_ngrams</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
<a id="__codelineno-16-8" name="__codelineno-16-8" href="#__codelineno-16-8"></a>    <span class="n">tokens</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span>
<a id="__codelineno-16-9" name="__codelineno-16-9" href="#__codelineno-16-9"></a>    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">ngrams</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
<a id="__codelineno-16-10" name="__codelineno-16-10" href="#__codelineno-16-10"></a>
<a id="__codelineno-16-11" name="__codelineno-16-11" href="#__codelineno-16-11"></a><span class="k">def</span> <span class="nf">ngram_to_vector</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
<a id="__codelineno-16-12" name="__codelineno-16-12" href="#__codelineno-16-12"></a>    <span class="n">n_grams</span> <span class="o">=</span> <span class="n">generate_ngrams</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
<a id="__codelineno-16-13" name="__codelineno-16-13" href="#__codelineno-16-13"></a>    <span class="n">n_gram_freq</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">n_grams</span><span class="p">)</span>
<a id="__codelineno-16-14" name="__codelineno-16-14" href="#__codelineno-16-14"></a>    <span class="k">return</span> <span class="n">n_gram_freq</span>
<a id="__codelineno-16-15" name="__codelineno-16-15" href="#__codelineno-16-15"></a>
<a id="__codelineno-16-16" name="__codelineno-16-16" href="#__codelineno-16-16"></a><span class="c1"># Example text</span>
<a id="__codelineno-16-17" name="__codelineno-16-17" href="#__codelineno-16-17"></a><span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;I love learning NLP from Ankamala tutorials&quot;</span>
<a id="__codelineno-16-18" name="__codelineno-16-18" href="#__codelineno-16-18"></a>
<a id="__codelineno-16-19" name="__codelineno-16-19" href="#__codelineno-16-19"></a><span class="c1"># Generate bigrams and trigrams</span>
<a id="__codelineno-16-20" name="__codelineno-16-20" href="#__codelineno-16-20"></a><span class="n">bigrams</span> <span class="o">=</span> <span class="n">ngram_to_vector</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<a id="__codelineno-16-21" name="__codelineno-16-21" href="#__codelineno-16-21"></a><span class="n">trigrams</span> <span class="o">=</span> <span class="n">ngram_to_vector</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<a id="__codelineno-16-22" name="__codelineno-16-22" href="#__codelineno-16-22"></a>
<a id="__codelineno-16-23" name="__codelineno-16-23" href="#__codelineno-16-23"></a><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Bigrams Vector:&quot;</span><span class="p">,</span> <span class="n">bigrams</span><span class="p">)</span>
<a id="__codelineno-16-24" name="__codelineno-16-24" href="#__codelineno-16-24"></a><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Trigrams Vector:&quot;</span><span class="p">,</span> <span class="n">trigrams</span><span class="p">)</span>
</code></pre></div>
<strong>Expected Output:</strong>
<div class="highlight"><pre><span></span><code><a id="__codelineno-17-1" name="__codelineno-17-1" href="#__codelineno-17-1"></a>Bigrams Vector: Counter({(&#39;i&#39;, &#39;love&#39;): 1, (&#39;love&#39;, &#39;learning&#39;): 1, (&#39;learning&#39;, &#39;nlp&#39;): 1, (&#39;nlp&#39;, &#39;from&#39;): 1, (&#39;from&#39;, &#39;ankamala&#39;): 1, (&#39;ankamala&#39;, &#39;tutorials&#39;): 1})
<a id="__codelineno-17-2" name="__codelineno-17-2" href="#__codelineno-17-2"></a>Trigrams Vector: Counter({(&#39;i&#39;, &#39;love&#39;, &#39;learning&#39;): 1, (&#39;love&#39;, &#39;learning&#39;, &#39;nlp&#39;): 1, (&#39;learning&#39;, &#39;nlp&#39;, &#39;from&#39;): 1, (&#39;nlp&#39;, &#39;from&#39;, &#39;ankamala&#39;): 1, (&#39;from&#39;, &#39;ankamala&#39;, &#39;tutorials&#39;): 1})
</code></pre></div>
<br/></p>
<h3 id="62-converting-input-to-vector-ii-advanced-approaches">6.2 Converting Input to Vector II (Advanced Approaches)</h3>
<h4 id="i-word2vec-iiaverage-word2vec">i. Word2Vec   &emsp;&emsp;     ii.Average Word2Vec</h4>
<p><br/><strong>i.Word2Vec</strong> 
 - Represents words in a vector space where similar words have closer vector representations.</p>
<h4 id="why-use-word2vec">Why Use Word2Vec?</h4>
<p>Word2Vec helps us understand the meaning of words by representing them as vectors in a space where similar words are close together. Unlike simpler methods like TF-IDF, Word2Vec captures word similarities and relationships better, making it useful for various NLP tasks.</p>
<h4 id="how-it-works">How it Works</h4>
<p>Speech recognition systems convert spoken language into text. Word embeddings like Word2Vec help these systems understand the context and meaning of the words in the text, making the recognition process more accurate.</p>
<ul>
<li>
<ol>
<li><strong>Introduction to Word Embeddings</strong></li>
</ol>
</li>
</ul>
<p>Word embeddings are vector representations of words that capture their meanings and relationships. They’re crucial for understanding language in a more nuanced way.</p>
<p>-2. ** Word2Vec with Gensim**</p>
<p>Gensim is a powerful library for working with Word2Vec. Here’s how you can use it:</p>
<p><strong>Install Gensim:</strong>
<div class="highlight"><pre><span></span><code><a id="__codelineno-18-1" name="__codelineno-18-1" href="#__codelineno-18-1"></a>pip install gensim
</code></pre></div>
<strong>Code Example</strong>:
<div class="highlight"><pre><span></span><code><a id="__codelineno-19-1" name="__codelineno-19-1" href="#__codelineno-19-1"></a><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">Word2Vec</span>
<a id="__codelineno-19-2" name="__codelineno-19-2" href="#__codelineno-19-2"></a><span class="kn">from</span> <span class="nn">gensim.utils</span> <span class="kn">import</span> <span class="n">simple_preprocess</span>
<a id="__codelineno-19-3" name="__codelineno-19-3" href="#__codelineno-19-3"></a>
<a id="__codelineno-19-4" name="__codelineno-19-4" href="#__codelineno-19-4"></a><span class="c1"># Sample sentences</span>
<a id="__codelineno-19-5" name="__codelineno-19-5" href="#__codelineno-19-5"></a><span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span>
<a id="__codelineno-19-6" name="__codelineno-19-6" href="#__codelineno-19-6"></a>    <span class="s2">&quot;This is a sample sentence.&quot;</span><span class="p">,</span>
<a id="__codelineno-19-7" name="__codelineno-19-7" href="#__codelineno-19-7"></a>    <span class="s2">&quot;This is another example of a sentence.&quot;</span><span class="p">,</span>
<a id="__codelineno-19-8" name="__codelineno-19-8" href="#__codelineno-19-8"></a>    <span class="s2">&quot;Natural language processing is fun.&quot;</span><span class="p">,</span>
<a id="__codelineno-19-9" name="__codelineno-19-9" href="#__codelineno-19-9"></a>    <span class="s2">&quot;Machine learning can be applied to text data.&quot;</span>
<a id="__codelineno-19-10" name="__codelineno-19-10" href="#__codelineno-19-10"></a><span class="p">]</span>
<a id="__codelineno-19-11" name="__codelineno-19-11" href="#__codelineno-19-11"></a>
<a id="__codelineno-19-12" name="__codelineno-19-12" href="#__codelineno-19-12"></a><span class="c1"># Tokenize and preprocess sentences</span>
<a id="__codelineno-19-13" name="__codelineno-19-13" href="#__codelineno-19-13"></a><span class="n">preprocessed_sentences</span> <span class="o">=</span> <span class="p">[</span><span class="n">simple_preprocess</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">]</span>
<a id="__codelineno-19-14" name="__codelineno-19-14" href="#__codelineno-19-14"></a>
<a id="__codelineno-19-15" name="__codelineno-19-15" href="#__codelineno-19-15"></a><span class="c1"># Train Word2Vec model</span>
<a id="__codelineno-19-16" name="__codelineno-19-16" href="#__codelineno-19-16"></a><span class="n">model</span> <span class="o">=</span> <span class="n">Word2Vec</span><span class="p">(</span><span class="n">sentences</span><span class="o">=</span><span class="n">preprocessed_sentences</span><span class="p">,</span> <span class="n">vector_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">sg</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<a id="__codelineno-19-17" name="__codelineno-19-17" href="#__codelineno-19-17"></a>
<a id="__codelineno-19-18" name="__codelineno-19-18" href="#__codelineno-19-18"></a><span class="c1"># Get word vectors</span>
<a id="__codelineno-19-19" name="__codelineno-19-19" href="#__codelineno-19-19"></a><span class="n">word_vectors</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span>
<a id="__codelineno-19-20" name="__codelineno-19-20" href="#__codelineno-19-20"></a>
<a id="__codelineno-19-21" name="__codelineno-19-21" href="#__codelineno-19-21"></a><span class="c1"># Display vector for a word</span>
<a id="__codelineno-19-22" name="__codelineno-19-22" href="#__codelineno-19-22"></a><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vector for &#39;sentence&#39;:&quot;</span><span class="p">,</span> <span class="n">word_vectors</span><span class="p">[</span><span class="s1">&#39;sentence&#39;</span><span class="p">])</span>
<a id="__codelineno-19-23" name="__codelineno-19-23" href="#__codelineno-19-23"></a>
<a id="__codelineno-19-24" name="__codelineno-19-24" href="#__codelineno-19-24"></a><span class="c1"># Find similar words</span>
<a id="__codelineno-19-25" name="__codelineno-19-25" href="#__codelineno-19-25"></a><span class="n">similar_words</span> <span class="o">=</span> <span class="n">word_vectors</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s1">&#39;sentence&#39;</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<a id="__codelineno-19-26" name="__codelineno-19-26" href="#__codelineno-19-26"></a><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Words similar to &#39;sentence&#39;:&quot;</span><span class="p">,</span> <span class="n">similar_words</span><span class="p">)</span>
</code></pre></div>
<strong>Expected Output:</strong>
<div class="highlight"><pre><span></span><code><a id="__codelineno-20-1" name="__codelineno-20-1" href="#__codelineno-20-1"></a>Vector for &#39;sentence&#39;: [ 0.0132551  -0.0017596  -0.0068934  -0.01502807  0.00064916  0.01293842
<a id="__codelineno-20-2" name="__codelineno-20-2" href="#__codelineno-20-2"></a> -0.00333229 ....]
<a id="__codelineno-20-3" name="__codelineno-20-3" href="#__codelineno-20-3"></a>
<a id="__codelineno-20-4" name="__codelineno-20-4" href="#__codelineno-20-4"></a>Words similar to &#39;sentence&#39;: [(&#39;sample&#39;, 0.998260498046875), (&#39;another&#39;, 0.9979914426803589), (&#39;example&#39;, 0.9976629014015198)]
</code></pre></div></p>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      &copy; 2024 <a href="https://ankamala.org" target="_blank" rel="noopener">Ankamala</a>

    </div>
  
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.sections", "toc.integrate", "navigation.top", "search.suggest", "search.highlight", "content.tabs.link", "content.code.annotation", "content.code.copy"], "search": "../../assets/javascripts/workers/search.07f07601.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.56dfad97.min.js"></script>
      
    
  </body>
</html>